{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/zxie52/AI_workshop_2024Summer/blob/main/2_1_programmatic_llm_elements.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "I8UpfZ5t4nQ6"
      },
      "source": [
        "<a target=\"_blank\" href=\"https://colab.research.google.com/github/vanderbilt-data-science/ai_summer/blob/main/2_1-programmatic-llm-elements.ipynb\">\n",
        "  <img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/>\n",
        "</a>\n",
        "\n",
        "# Programmatic LLM Elements\n",
        "> For Vanderbilt University AI Summer 2024 Prepared by Dr. Charreau Bell\n",
        "\n",
        "_Code versions applicable: May 13, 2024_\n",
        "\n",
        "## Learning Outcomes:\n",
        "* Participants will be able to explain the core messaging elements of programmatic interaction with ChatLLMs, specifically system messages, user messages, and assistant messages.\n",
        "* Participants will be able to explain the programmatic requirements of conversational AI with completions-like APIs in relationship to context and chat history.\n",
        "* Participants will be able to integrate additional parameters of LLM API calls to modify the default behavior of the LLMs.\n",
        "* Participants will understand the usage of APIs within demonstrative applications."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uQbz868wYDQr"
      },
      "source": [
        "### Setup\n",
        "Here, we'll prepare the coding environment with packages and API keys. This notebook assumes Google Colab."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0XrpXhflrB2k",
        "collapsed": true,
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "c1267e49-c87a-45cf-b68f-c3af0ca04fbc"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: openai in /usr/local/lib/python3.10/dist-packages (1.28.1)\n",
            "Requirement already satisfied: gradio in /usr/local/lib/python3.10/dist-packages (4.31.0)\n",
            "Requirement already satisfied: anyio<5,>=3.5.0 in /usr/local/lib/python3.10/dist-packages (from openai) (3.7.1)\n",
            "Requirement already satisfied: distro<2,>=1.7.0 in /usr/lib/python3/dist-packages (from openai) (1.7.0)\n",
            "Requirement already satisfied: httpx<1,>=0.23.0 in /usr/local/lib/python3.10/dist-packages (from openai) (0.27.0)\n",
            "Requirement already satisfied: pydantic<3,>=1.9.0 in /usr/local/lib/python3.10/dist-packages (from openai) (2.7.1)\n",
            "Requirement already satisfied: sniffio in /usr/local/lib/python3.10/dist-packages (from openai) (1.3.1)\n",
            "Requirement already satisfied: tqdm>4 in /usr/local/lib/python3.10/dist-packages (from openai) (4.66.4)\n",
            "Requirement already satisfied: typing-extensions<5,>=4.7 in /usr/local/lib/python3.10/dist-packages (from openai) (4.11.0)\n",
            "Requirement already satisfied: aiofiles<24.0,>=22.0 in /usr/local/lib/python3.10/dist-packages (from gradio) (23.2.1)\n",
            "Requirement already satisfied: altair<6.0,>=4.2.0 in /usr/local/lib/python3.10/dist-packages (from gradio) (4.2.2)\n",
            "Requirement already satisfied: fastapi in /usr/local/lib/python3.10/dist-packages (from gradio) (0.111.0)\n",
            "Requirement already satisfied: ffmpy in /usr/local/lib/python3.10/dist-packages (from gradio) (0.3.2)\n",
            "Requirement already satisfied: gradio-client==0.16.2 in /usr/local/lib/python3.10/dist-packages (from gradio) (0.16.2)\n",
            "Requirement already satisfied: huggingface-hub>=0.19.3 in /usr/local/lib/python3.10/dist-packages (from gradio) (0.20.3)\n",
            "Requirement already satisfied: importlib-resources<7.0,>=1.3 in /usr/local/lib/python3.10/dist-packages (from gradio) (6.4.0)\n",
            "Requirement already satisfied: jinja2<4.0 in /usr/local/lib/python3.10/dist-packages (from gradio) (3.1.4)\n",
            "Requirement already satisfied: markupsafe~=2.0 in /usr/local/lib/python3.10/dist-packages (from gradio) (2.1.5)\n",
            "Requirement already satisfied: matplotlib~=3.0 in /usr/local/lib/python3.10/dist-packages (from gradio) (3.7.1)\n",
            "Requirement already satisfied: numpy~=1.0 in /usr/local/lib/python3.10/dist-packages (from gradio) (1.25.2)\n",
            "Requirement already satisfied: orjson~=3.0 in /usr/local/lib/python3.10/dist-packages (from gradio) (3.10.3)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.10/dist-packages (from gradio) (24.0)\n",
            "Requirement already satisfied: pandas<3.0,>=1.0 in /usr/local/lib/python3.10/dist-packages (from gradio) (2.0.3)\n",
            "Requirement already satisfied: pillow<11.0,>=8.0 in /usr/local/lib/python3.10/dist-packages (from gradio) (9.4.0)\n",
            "Requirement already satisfied: pydub in /usr/local/lib/python3.10/dist-packages (from gradio) (0.25.1)\n",
            "Requirement already satisfied: python-multipart>=0.0.9 in /usr/local/lib/python3.10/dist-packages (from gradio) (0.0.9)\n",
            "Requirement already satisfied: pyyaml<7.0,>=5.0 in /usr/local/lib/python3.10/dist-packages (from gradio) (6.0.1)\n",
            "Requirement already satisfied: ruff>=0.2.2 in /usr/local/lib/python3.10/dist-packages (from gradio) (0.4.4)\n",
            "Requirement already satisfied: semantic-version~=2.0 in /usr/local/lib/python3.10/dist-packages (from gradio) (2.10.0)\n",
            "Requirement already satisfied: tomlkit==0.12.0 in /usr/local/lib/python3.10/dist-packages (from gradio) (0.12.0)\n",
            "Requirement already satisfied: typer<1.0,>=0.12 in /usr/local/lib/python3.10/dist-packages (from gradio) (0.12.3)\n",
            "Requirement already satisfied: urllib3~=2.0 in /usr/local/lib/python3.10/dist-packages (from gradio) (2.0.7)\n",
            "Requirement already satisfied: uvicorn>=0.14.0 in /usr/local/lib/python3.10/dist-packages (from gradio) (0.29.0)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.10/dist-packages (from gradio-client==0.16.2->gradio) (2023.6.0)\n",
            "Requirement already satisfied: websockets<12.0,>=10.0 in /usr/local/lib/python3.10/dist-packages (from gradio-client==0.16.2->gradio) (11.0.3)\n",
            "Requirement already satisfied: entrypoints in /usr/local/lib/python3.10/dist-packages (from altair<6.0,>=4.2.0->gradio) (0.4)\n",
            "Requirement already satisfied: jsonschema>=3.0 in /usr/local/lib/python3.10/dist-packages (from altair<6.0,>=4.2.0->gradio) (4.19.2)\n",
            "Requirement already satisfied: toolz in /usr/local/lib/python3.10/dist-packages (from altair<6.0,>=4.2.0->gradio) (0.12.1)\n",
            "Requirement already satisfied: idna>=2.8 in /usr/local/lib/python3.10/dist-packages (from anyio<5,>=3.5.0->openai) (3.7)\n",
            "Requirement already satisfied: exceptiongroup in /usr/local/lib/python3.10/dist-packages (from anyio<5,>=3.5.0->openai) (1.2.1)\n",
            "Requirement already satisfied: certifi in /usr/local/lib/python3.10/dist-packages (from httpx<1,>=0.23.0->openai) (2024.2.2)\n",
            "Requirement already satisfied: httpcore==1.* in /usr/local/lib/python3.10/dist-packages (from httpx<1,>=0.23.0->openai) (1.0.5)\n",
            "Requirement already satisfied: h11<0.15,>=0.13 in /usr/local/lib/python3.10/dist-packages (from httpcore==1.*->httpx<1,>=0.23.0->openai) (0.14.0)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from huggingface-hub>=0.19.3->gradio) (3.14.0)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from huggingface-hub>=0.19.3->gradio) (2.31.0)\n",
            "Requirement already satisfied: contourpy>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib~=3.0->gradio) (1.2.1)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.10/dist-packages (from matplotlib~=3.0->gradio) (0.12.1)\n",
            "Requirement already satisfied: fonttools>=4.22.0 in /usr/local/lib/python3.10/dist-packages (from matplotlib~=3.0->gradio) (4.51.0)\n",
            "Requirement already satisfied: kiwisolver>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib~=3.0->gradio) (1.4.5)\n",
            "Requirement already satisfied: pyparsing>=2.3.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib~=3.0->gradio) (3.1.2)\n",
            "Requirement already satisfied: python-dateutil>=2.7 in /usr/local/lib/python3.10/dist-packages (from matplotlib~=3.0->gradio) (2.8.2)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.10/dist-packages (from pandas<3.0,>=1.0->gradio) (2023.4)\n",
            "Requirement already satisfied: tzdata>=2022.1 in /usr/local/lib/python3.10/dist-packages (from pandas<3.0,>=1.0->gradio) (2024.1)\n",
            "Requirement already satisfied: annotated-types>=0.4.0 in /usr/local/lib/python3.10/dist-packages (from pydantic<3,>=1.9.0->openai) (0.6.0)\n",
            "Requirement already satisfied: pydantic-core==2.18.2 in /usr/local/lib/python3.10/dist-packages (from pydantic<3,>=1.9.0->openai) (2.18.2)\n",
            "Requirement already satisfied: click>=8.0.0 in /usr/local/lib/python3.10/dist-packages (from typer<1.0,>=0.12->gradio) (8.1.7)\n",
            "Requirement already satisfied: shellingham>=1.3.0 in /usr/local/lib/python3.10/dist-packages (from typer<1.0,>=0.12->gradio) (1.5.4)\n",
            "Requirement already satisfied: rich>=10.11.0 in /usr/local/lib/python3.10/dist-packages (from typer<1.0,>=0.12->gradio) (13.7.1)\n",
            "Requirement already satisfied: starlette<0.38.0,>=0.37.2 in /usr/local/lib/python3.10/dist-packages (from fastapi->gradio) (0.37.2)\n",
            "Requirement already satisfied: fastapi-cli>=0.0.2 in /usr/local/lib/python3.10/dist-packages (from fastapi->gradio) (0.0.3)\n",
            "Requirement already satisfied: ujson!=4.0.2,!=4.1.0,!=4.2.0,!=4.3.0,!=5.0.0,!=5.1.0,>=4.0.1 in /usr/local/lib/python3.10/dist-packages (from fastapi->gradio) (5.9.0)\n",
            "Requirement already satisfied: email_validator>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from fastapi->gradio) (2.1.1)\n",
            "Requirement already satisfied: dnspython>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from email_validator>=2.0.0->fastapi->gradio) (2.6.1)\n",
            "Requirement already satisfied: attrs>=22.2.0 in /usr/local/lib/python3.10/dist-packages (from jsonschema>=3.0->altair<6.0,>=4.2.0->gradio) (23.2.0)\n",
            "Requirement already satisfied: jsonschema-specifications>=2023.03.6 in /usr/local/lib/python3.10/dist-packages (from jsonschema>=3.0->altair<6.0,>=4.2.0->gradio) (2023.12.1)\n",
            "Requirement already satisfied: referencing>=0.28.4 in /usr/local/lib/python3.10/dist-packages (from jsonschema>=3.0->altair<6.0,>=4.2.0->gradio) (0.35.1)\n",
            "Requirement already satisfied: rpds-py>=0.7.1 in /usr/local/lib/python3.10/dist-packages (from jsonschema>=3.0->altair<6.0,>=4.2.0->gradio) (0.18.1)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.10/dist-packages (from python-dateutil>=2.7->matplotlib~=3.0->gradio) (1.16.0)\n",
            "Requirement already satisfied: markdown-it-py>=2.2.0 in /usr/local/lib/python3.10/dist-packages (from rich>=10.11.0->typer<1.0,>=0.12->gradio) (3.0.0)\n",
            "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in /usr/local/lib/python3.10/dist-packages (from rich>=10.11.0->typer<1.0,>=0.12->gradio) (2.16.1)\n",
            "Requirement already satisfied: httptools>=0.5.0 in /usr/local/lib/python3.10/dist-packages (from uvicorn>=0.14.0->gradio) (0.6.1)\n",
            "Requirement already satisfied: python-dotenv>=0.13 in /usr/local/lib/python3.10/dist-packages (from uvicorn>=0.14.0->gradio) (1.0.1)\n",
            "Requirement already satisfied: uvloop!=0.15.0,!=0.15.1,>=0.14.0 in /usr/local/lib/python3.10/dist-packages (from uvicorn>=0.14.0->gradio) (0.19.0)\n",
            "Requirement already satisfied: watchfiles>=0.13 in /usr/local/lib/python3.10/dist-packages (from uvicorn>=0.14.0->gradio) (0.21.0)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface-hub>=0.19.3->gradio) (3.3.2)\n",
            "Requirement already satisfied: mdurl~=0.1 in /usr/local/lib/python3.10/dist-packages (from markdown-it-py>=2.2.0->rich>=10.11.0->typer<1.0,>=0.12->gradio) (0.1.2)\n"
          ]
        }
      ],
      "source": [
        "# install from pypi\n",
        "! pip install openai gradio"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "rwOTZ-RZYDQu"
      },
      "outputs": [],
      "source": [
        "# standard practice is all imports at the top, but for learning purposes, this is distributed throughout the notebook\n",
        "import os"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "agUgynklrpuG"
      },
      "outputs": [],
      "source": [
        "# make available to python\n",
        "from google.colab import userdata"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jWyV7tAM4wKp"
      },
      "source": [
        "Here, you need to make sure that you have added your OpenAI API key:\n",
        "* Go to: https://platform.openai.com\n",
        "* Make sure that you are logged in\n",
        "* Go to sidebar API keys\n",
        "* Follow instructions\n",
        "* SAVE YOUR API KEY AS YOU WILL NEVER SEE IT WRITTEN AGAIN\n",
        "* Add it to the Colab sidebar with name `OPENAI_API_KEY`"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Z-HGLB4_4-aN"
      },
      "outputs": [],
      "source": [
        "# set environment variable to be used by openAI client\n",
        "os.environ['OPENAI_API_KEY'] = userdata.get('ZengboXie')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cJyjyFmdYDQw"
      },
      "source": [
        "## Programmatic LLM API Elements\n",
        "Resources:\n",
        "* [OpenAI API Reference](https://platform.openai.com/docs/overview)\n",
        "* [Direct Link to Completions Quickstart](https://platform.openai.com/docs/quickstart)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HNImLdG_YDQw"
      },
      "source": [
        "### Get completion: From OpenAI Quickstart"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "McZ09jJz1Zrf",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "99d7388c-abaa-4117-ab13-2d9333e3d20c"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "ChatCompletionMessage(content='Hello! How can I assist you today?', role='assistant', function_call=None, tool_calls=None)\n"
          ]
        }
      ],
      "source": [
        "# Copy from Quickstart\n",
        "from openai import OpenAI\n",
        "client = OpenAI()\n",
        "\n",
        "completion = client.chat.completions.create(\n",
        "  model=\"gpt-3.5-turbo\",\n",
        "  messages=[\n",
        "    {\"role\": \"system\", \"content\": \"You are a helpful assistant.\"},\n",
        "    {\"role\": \"user\", \"content\": \"Hello!\"}\n",
        "  ]\n",
        ")\n",
        "\n",
        "print(completion.choices[0].message)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jWTM2x2LYDQx",
        "outputId": "a70ffcb2-d7be-44fe-e1c5-5deeef586d4e"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'id': 'chatcmpl-9ORedycPNvmWXsUA3MZPd804xLr12',\n",
              " 'choices': [{'finish_reason': 'stop',\n",
              "   'index': 0,\n",
              "   'logprobs': None,\n",
              "   'message': {'content': 'Hello! How can I assist you today?',\n",
              "    'role': 'assistant',\n",
              "    'function_call': None,\n",
              "    'tool_calls': None}}],\n",
              " 'created': 1715613203,\n",
              " 'model': 'gpt-3.5-turbo-0125',\n",
              " 'object': 'chat.completion',\n",
              " 'system_fingerprint': None,\n",
              " 'usage': {'completion_tokens': 9, 'prompt_tokens': 19, 'total_tokens': 28}}"
            ]
          },
          "metadata": {},
          "execution_count": 24
        }
      ],
      "source": [
        "# view structure of completion\n",
        "completion.model_dump()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZUqJp9KMYDQx",
        "outputId": "e021a86a-e7d1-449e-cc7a-62ceefe66c84"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Hello! How can I assist you today?\n",
            "Hello! How can I assist you today?\n"
          ]
        }
      ],
      "source": [
        "# obtain the string content of the completion\n",
        "print(completion.choices[0].message.content)\n",
        "\n",
        "# save into variable just for convenience\n",
        "completion_1 = completion.choices[0].message.content\n",
        "print(completion_1)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HKs2HMu9YDQx"
      },
      "source": [
        "### Updating the conversational context from response\n",
        "Tip: If developing locally, debuggers are your best friend!"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "GJ_LfnKY1c1F"
      },
      "outputs": [],
      "source": [
        "completion = client.chat.completions.create(\n",
        "  model=\"gpt-3.5-turbo\",\n",
        "  messages=[\n",
        "    {\"role\": \"system\", \"content\": \"You are a poetic assistant, skilled in explaining complex programming concepts with creative flair.\"},\n",
        "    {\"role\": \"user\", \"content\": \"Compose a poem that explains the concept of recursion in programming.\"},\n",
        "    # add updates to conversation here\n",
        "    {\"role\": \"assistant\", \"content\": completion_1},\n",
        "    {'role': 'user', 'content': \"You didnt tell me anything about recursion\"}\n",
        "\n",
        "  ]\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "OMIJB_x2YDQy",
        "outputId": "db218e46-75ff-4763-e8b2-3cf22d02e790"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'id': 'chatcmpl-9ORjiMg1t3xFSrsQNJTIHb1I9s4Q0',\n",
              " 'choices': [{'finish_reason': 'stop',\n",
              "   'index': 0,\n",
              "   'logprobs': None,\n",
              "   'message': {'content': \"On wings of code, a tale unfolds,\\nA concept deep, a story told.\\nRecursion dances, in loops it twirls,\\nUnleashing magic, in the coding world.\\n\\nA function calls itself, brave and bold,\\nInto the abyss of tasks untold.\\nLike a mirror reflecting its own gaze,\\nRecursion loops in a mystic maze.\\n\\nWith each call, a problem is split,\\nInto smaller pieces, bit by bit.\\nUntil a base case, like a shining star,\\nEnds the journey, near or far.\\n\\nInfinite patterns, in a loop's embrace,\\nRecursion reveals its wondrous grace.\\nA never-ending dance, a poetic rhyme,\\nIn the realm of code, through space and time.\",\n",
              "    'role': 'assistant',\n",
              "    'function_call': None,\n",
              "    'tool_calls': None}}],\n",
              " 'created': 1715613518,\n",
              " 'model': 'gpt-3.5-turbo-0125',\n",
              " 'object': 'chat.completion',\n",
              " 'system_fingerprint': None,\n",
              " 'usage': {'completion_tokens': 146, 'prompt_tokens': 63, 'total_tokens': 209}}"
            ]
          },
          "metadata": {},
          "execution_count": 36
        }
      ],
      "source": [
        "# view the response\n",
        "completion.model_dump()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "PNTnyNJk17UC",
        "outputId": "85b17bc7-dfc2-41f0-9bf5-0399589a6374"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "On wings of code, a tale unfolds,\n",
            "A concept deep, a story told.\n",
            "Recursion dances, in loops it twirls,\n",
            "Unleashing magic, in the coding world.\n",
            "\n",
            "A function calls itself, brave and bold,\n",
            "Into the abyss of tasks untold.\n",
            "Like a mirror reflecting its own gaze,\n",
            "Recursion loops in a mystic maze.\n",
            "\n",
            "With each call, a problem is split,\n",
            "Into smaller pieces, bit by bit.\n",
            "Until a base case, like a shining star,\n",
            "Ends the journey, near or far.\n",
            "\n",
            "Infinite patterns, in a loop's embrace,\n",
            "Recursion reveals its wondrous grace.\n",
            "A never-ending dance, a poetic rhyme,\n",
            "In the realm of code, through space and time.\n"
          ]
        }
      ],
      "source": [
        "# just the text\n",
        "completion_2 = completion.choices[0].message.content\n",
        "print(f\"{completion.choices[0].message.content}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jONCeBObYDQy"
      },
      "source": [
        "### On your own: Continuing the conversation\n",
        "Now, continue the conversation based on the previous response. Your new question should be based on something about python dictionaries without specifically referencing it. Some examples:\n",
        "* \"Show me an example of a comprehension based on this.\"\n",
        "* \"How does this differ from a python list?\"\n",
        "\n",
        "View the response as a string, and not the total response object."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hfUAD7dn2KjW",
        "outputId": "6b503e85-f478-4119-f1c8-cb132c7f15aa"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Of course! Here's a Python code snippet that demonstrates recursion with a function that calculates the factorial of a number:\n",
            "\n",
            "```python\n",
            "def factorial(n):\n",
            "    # Base case: if n is 0 or 1, return 1\n",
            "    if n == 0 or n == 1:\n",
            "        return 1\n",
            "    else:\n",
            "        # Recursive call: n! = n * (n-1)!\n",
            "        return n * factorial(n - 1)\n",
            "\n",
            "# Calling the factorial function with n = 5\n",
            "result = factorial(5)\n",
            "print(result)  # Output: 120\n",
            "```\n",
            "\n",
            "In this example, the `factorial` function recursively calculates the factorial of a number `n` by multiplying `n` with the factorial of `n-1`. The recursion continues until it reaches the base case of `n = 0` or `n = 1`, where the function returns 1 to terminate the recursive calls.\n"
          ]
        }
      ],
      "source": [
        "# Continue the conversation\n",
        "completion = client.chat.completions.create(\n",
        "  model=\"gpt-3.5-turbo\",\n",
        "  messages=[\n",
        "    {\"role\": \"system\", \"content\": \"You are a poetic assistant, skilled in explaining complex programming concepts with creative flair.\"},\n",
        "    {\"role\": \"user\", \"content\": \"Compose a poem that explains the concept of recursion in programming.\"},\n",
        "    {\"role\": \"assistant\", \"content\": completion_1},\n",
        "    {\"role\": \"user\", \"content\": \"Now explain python dictionaries.\"},\n",
        "    {\"role\": \"assistant\", \"content\": completion_2},\n",
        "    {\"role\": \"user\", \"content\": \"Can you give a code example on this?\"}\n",
        "  ]\n",
        ")\n",
        "\n",
        "# view the response as a string\n",
        "print(completion.choices[0].message.content)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "N9lf7bshYDQz"
      },
      "source": [
        "## APIs: An exploration using OpenAI's Chat Completions API\n",
        "APIs are contracts between developers and services"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wy8ol6TTYDQz"
      },
      "source": [
        "### APIs: Example 1 - Limiting Tokens"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tez00leDYDQz",
        "outputId": "3de6c8b0-1f97-4d98-c3a7-800a7a3df6d5"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'id': 'chatcmpl-9OSFhGUshN2RUTCCKWVivclJGJOn0',\n",
              " 'choices': [{'finish_reason': 'stop',\n",
              "   'index': 0,\n",
              "   'logprobs': None,\n",
              "   'message': {'content': 'Once upon a time, in a cozy little cottage at the edge of a bustling town, there lived a mischievous cat named Whiskers. Whiskers was a fluffy orange tabby with bright green eyes and a playful spirit that often got him into trouble. However, his most dreaded moment was when his mommy, Mrs. Jenkins, needed to trim his sharp claws.\\n\\nMrs. Jenkins loved Whiskers dearly and always tried to keep his claws trimmed to prevent any accidents around the house. But the moment Whiskers saw the nail clippers in her hand, he knew it was time to play his favorite game – the hiding game.\\n\\nAs soon as Whiskers caught sight of the nail clippers, he would dart off, his paws thudding softly on the wooden floors as he sought out the perfect hiding spot. Mrs. Jenkins would call out to him, \"Whiskers, it\\'s time for your nail trim,\" but Whiskers paid no mind. He was determined to outwit his mommy once again.\\n\\nOne day, Mrs. Jenkins found Whiskers nestled inside the laundry basket, hidden among the soft, warm towels. With a chuckle, she reached in and gently scooped him up. But Whiskers was not daunted. The next time, he climbed up to the highest shelf in the pantry, nestled between the cans of cat food, thinking himself clever.\\n\\nAnother time, he squeezed himself into the empty flower pot on the windowsill, his fluffy tail poking out comically. Mrs. Jenkins shook her head in amusement as she reached to retrieve him.\\n\\nBut Whiskers\\' most elaborate hiding spot was behind the grandfather clock in the living room. With great finesse, he squeezed his way into the narrow space between the clock and the wall, his eyes wide with excitement at his successful hiding spot.\\n\\nMrs. Jenkins had to stifle a laugh when she spotted him there, his whiskers twitching in anticipation. She kneeled down and gently coaxed him out from behind the clock, her voice full of affection, \"Oh, Whiskers, you are truly the master of hide and seek.\"\\n\\nAs Whiskers finally emerged from his hiding place, he gave a contented purr, knowing that he had once again outsmarted his mommy. Mrs. Jenkins smiled fondly at her mischievous cat, realizing that even though Whiskers made nail-trimming a challenge, his playful antics brought joy and laughter into their home.\\n\\nAnd so, the playful game of hide and seek continued between Whiskers and his mommy, each nail-trimming session turning into a delightful adventure filled with laughter and love in their cozy little cottage.',\n",
              "    'role': 'assistant',\n",
              "    'function_call': None,\n",
              "    'tool_calls': None}}],\n",
              " 'created': 1715615501,\n",
              " 'model': 'gpt-3.5-turbo-0125',\n",
              " 'object': 'chat.completion',\n",
              " 'system_fingerprint': None,\n",
              " 'usage': {'completion_tokens': 544, 'prompt_tokens': 59, 'total_tokens': 603}}"
            ]
          },
          "metadata": {},
          "execution_count": 43
        }
      ],
      "source": [
        "conversation_messages = [\n",
        "    {\"role\": \"system\", \"content\": \"You are a fantastic storyteller and are an expert in telling long, engaging, highly descriptive, imaginative stories.\"},\n",
        "    {\"role\": \"user\", \"content\": \"Tell me a story about an extremely mischievous cat who finds creative places to hide whenever his mommy needs to cut his nails.\"}\n",
        "]\n",
        "\n",
        "full_token_completion = client.chat.completions.create(\n",
        "  model=\"gpt-3.5-turbo\",\n",
        "  messages=conversation_messages,\n",
        "  # add other parameters\n",
        "  max_tokens = None\n",
        "\n",
        ")\n",
        "\n",
        "full_token_completion.model_dump()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "NpcQC0HmYDQz",
        "outputId": "c9884020-2f07-4b8b-d549-b4ef176b38dd"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'stop'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 44
        }
      ],
      "source": [
        "# show reason for ending\n",
        "full_token_completion.choices[0].finish_reason"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dhas6bDsYDQz",
        "outputId": "c1707be9-c588-4902-9469-ba8e6a439b40"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'id': 'chatcmpl-9OSI5ZGvDkgDwbp7XjrAD8J1l87M3',\n",
              " 'choices': [{'finish_reason': 'length',\n",
              "   'index': 0,\n",
              "   'logprobs': None,\n",
              "   'message': {'content': 'In a quaint little town nestled between rolling green hills and babbling brooks, there lived a cat',\n",
              "    'role': 'assistant',\n",
              "    'function_call': None,\n",
              "    'tool_calls': None}}],\n",
              " 'created': 1715615649,\n",
              " 'model': 'gpt-3.5-turbo-0125',\n",
              " 'object': 'chat.completion',\n",
              " 'system_fingerprint': None,\n",
              " 'usage': {'completion_tokens': 20, 'prompt_tokens': 59, 'total_tokens': 79}}"
            ]
          },
          "metadata": {},
          "execution_count": 45
        }
      ],
      "source": [
        "# add in specific valuef or max_tokens parameter\n",
        "max_token_completion = client.chat.completions.create(\n",
        "  model=\"gpt-3.5-turbo\",\n",
        "  messages=conversation_messages,\n",
        "  # add other parameters\n",
        "  max_tokens = 20\n",
        ")\n",
        "\n",
        "max_token_completion.model_dump()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "X7mj8PI8YDQz",
        "outputId": "65b051ae-4dfe-4a5c-f9c3-ae192613ded3"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'length'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 46
        }
      ],
      "source": [
        "# show reason for ending\n",
        "max_token_completion.choices[0].finish_reason"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hOYL3y0tYDQ0"
      },
      "source": [
        "### APIs: Example 2 - Drilldown Inputs\n",
        "Sometimes you'll need to navigate deep into the API to get the information you need."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 382
        },
        "id": "832py0qiYDQ0",
        "outputId": "f9bdc0b2-3421-419d-9eac-14398e40fead"
      },
      "outputs": [
        {
          "output_type": "error",
          "ename": "BadRequestError",
          "evalue": "Error code: 400 - {'error': {'message': \"Invalid type for 'messages[0].content[1].image_url': expected an object, but got a string instead.\", 'type': 'invalid_request_error', 'param': 'messages[0].content[1].image_url', 'code': 'invalid_type'}}",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mBadRequestError\u001b[0m                           Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-48-98435df90427>\u001b[0m in \u001b[0;36m<cell line: 6>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0mclient\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mOpenAI\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 6\u001b[0;31m response = client.chat.completions.create(\n\u001b[0m\u001b[1;32m      7\u001b[0m     \u001b[0mmodel\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"gpt-3.5-turbo\"\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m     messages=[\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/openai/_utils/_utils.py\u001b[0m in \u001b[0;36mwrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    275\u001b[0m                         \u001b[0mmsg\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34mf\"Missing required argument: {quote(missing[0])}\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    276\u001b[0m                 \u001b[0;32mraise\u001b[0m \u001b[0mTypeError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmsg\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 277\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    278\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    279\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mwrapper\u001b[0m  \u001b[0;31m# type: ignore\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/openai/resources/chat/completions.py\u001b[0m in \u001b[0;36mcreate\u001b[0;34m(self, messages, model, frequency_penalty, function_call, functions, logit_bias, logprobs, max_tokens, n, presence_penalty, response_format, seed, stop, stream, stream_options, temperature, tool_choice, tools, top_logprobs, top_p, user, extra_headers, extra_query, extra_body, timeout)\u001b[0m\n\u001b[1;32m    588\u001b[0m         \u001b[0mtimeout\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mfloat\u001b[0m \u001b[0;34m|\u001b[0m \u001b[0mhttpx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTimeout\u001b[0m \u001b[0;34m|\u001b[0m \u001b[0;32mNone\u001b[0m \u001b[0;34m|\u001b[0m \u001b[0mNotGiven\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mNOT_GIVEN\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    589\u001b[0m     ) -> ChatCompletion | Stream[ChatCompletionChunk]:\n\u001b[0;32m--> 590\u001b[0;31m         return self._post(\n\u001b[0m\u001b[1;32m    591\u001b[0m             \u001b[0;34m\"/chat/completions\"\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    592\u001b[0m             body=maybe_transform(\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/openai/_base_client.py\u001b[0m in \u001b[0;36mpost\u001b[0;34m(self, path, cast_to, body, options, files, stream, stream_cls)\u001b[0m\n\u001b[1;32m   1238\u001b[0m             \u001b[0mmethod\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"post\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0murl\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mjson_data\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mbody\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfiles\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mto_httpx_files\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfiles\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0moptions\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1239\u001b[0m         )\n\u001b[0;32m-> 1240\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mcast\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mResponseT\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrequest\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcast_to\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mopts\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstream\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mstream\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstream_cls\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mstream_cls\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1241\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1242\u001b[0m     def patch(\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/openai/_base_client.py\u001b[0m in \u001b[0;36mrequest\u001b[0;34m(self, cast_to, options, remaining_retries, stream, stream_cls)\u001b[0m\n\u001b[1;32m    919\u001b[0m         \u001b[0mstream_cls\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mtype\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0m_StreamT\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m|\u001b[0m \u001b[0;32mNone\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    920\u001b[0m     ) -> ResponseT | _StreamT:\n\u001b[0;32m--> 921\u001b[0;31m         return self._request(\n\u001b[0m\u001b[1;32m    922\u001b[0m             \u001b[0mcast_to\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcast_to\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    923\u001b[0m             \u001b[0moptions\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0moptions\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/openai/_base_client.py\u001b[0m in \u001b[0;36m_request\u001b[0;34m(self, cast_to, options, remaining_retries, stream, stream_cls)\u001b[0m\n\u001b[1;32m   1018\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1019\u001b[0m             \u001b[0mlog\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdebug\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Re-raising status error\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1020\u001b[0;31m             \u001b[0;32mraise\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_make_status_error_from_response\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0merr\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mresponse\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1021\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1022\u001b[0m         return self._process_response(\n",
            "\u001b[0;31mBadRequestError\u001b[0m: Error code: 400 - {'error': {'message': \"Invalid type for 'messages[0].content[1].image_url': expected an object, but got a string instead.\", 'type': 'invalid_request_error', 'param': 'messages[0].content[1].image_url', 'code': 'invalid_type'}}"
          ]
        }
      ],
      "source": [
        "# Try completions with images\n",
        "from openai import OpenAI\n",
        "\n",
        "client = OpenAI()\n",
        "\n",
        "response = client.chat.completions.create(\n",
        "    model=\"gpt-3.5-turbo\",\n",
        "    messages=[\n",
        "        {\n",
        "            \"role\": \"user\",\n",
        "            \"content\": [\n",
        "                {\"type\": \"text\", \"text\": \"What's in this image?\"},\n",
        "                {\n",
        "                    \"type\": \"image_url\",\n",
        "                    \"image_url\": \"https://upload.wikimedia.org/wikipedia/commons/thumb/d/dd/Gfp-wisconsin-madison-the-nature-boardwalk.jpg/2560px-Gfp-wisconsin-madison-the-nature-boardwalk.jpg\",\n",
        "                },\n",
        "            ],\n",
        "        }\n",
        "    ],\n",
        "    max_tokens=300,\n",
        ")\n",
        "\n",
        "\n",
        "image_response.model_dump()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "D26CSxRQYDQ0"
      },
      "source": [
        "### Breakout Room: Chat Completions API (10 minutes)\n",
        "In this breakout room, you're going to explore implementing different parameters of the Chat Completions API. It has lots of functionality, and you can explore it based on your interests and those of your lab/group partners.\n",
        "\n",
        "In your breakout room:\n",
        "1. Choose a group leader to share your screen, and choose a writer who will document the results of the exploration.\n",
        "\n",
        "**[Creating Chat Completions](https://platform.openai.com/docs/api-reference/chat/create)**\n",
        "  * Read over all of the parameters that can be used in the request. Other than messages and model, which 3 parameters seem most interesting/useful for your purposes? Why?\n",
        "  * Choose a straightforward parameter of interest (e.g., temperature, seed, presence_penalty) and test it out with a simple prompt. What happens when you change the value of this parameter?\n",
        "  * [If time] Repeat the above with another straightforward parameter of interest.\n",
        "  * [Optional] Repeat the above with another parameter of interest, particularly if you are interested in `logprobs`, `response_format`, etc. We will work with tools another day.\n",
        "\n",
        "**[The chat completion object](https://platform.openai.com/docs/api-reference/chat/object)**\n",
        "  * What is the structure of choices? How would you access choice responses for n>1? Implement this below.\n",
        "  * What is the system fingerprint? In what cases do you think it would be useful?\n",
        "\n",
        "**[Extra Credit: The Moderation API](https://platform.openai.com/docs/api-reference/moderations)**\n",
        "  * What is the purpose of the moderation API? How might it be useful in a chatbot context?\n",
        "  * What models are available to do moderation? Do they appear to be GPT (LLM-based) models based on the models available and language of the API? Justify your answer and speculate in its implications/reasoning.\n",
        "  * Examine the example request given in the API documentation and try it out here."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vsHGcSSmYDQ0"
      },
      "source": [
        "#### Sample Code Solutions"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "xPvE0oACYDQ0"
      },
      "outputs": [],
      "source": [
        "# chat completions implementation 1, object 1\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "LD-4LHcyYDQ0"
      },
      "outputs": [],
      "source": [
        "# using moderations API (call your variable `moderation` and the code below will work)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "s8R6j5EZYDQ0"
      },
      "outputs": [],
      "source": [
        "# moderations viewing helper\n",
        "import pandas as pd\n",
        "df = pd.DataFrame({'category':moderation.results[0].categories.model_dump(),\n",
        "                   'score':moderation.results[0].category_scores.model_dump()})\n",
        "df.index = df.index.str.replace('/', '_')\n",
        "df.drop_duplicates().sort_values(by=['category', 'score'], ascending=False)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AmUpQUo_YDQ0"
      },
      "source": [
        "## Foundational behavior vs application behavior\n",
        "The code above is foundational programmatic access of LLMs (in OpenAI). Then, you add programming around it to actually make it useful.\n",
        "\n",
        "### \"Command line\" interaction\n",
        "Below is the functionality that keeps track of the history and makes the API calls."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "sp9hOtI7YDQ1"
      },
      "outputs": [],
      "source": [
        "# This is an example of a function that can help to update chat history\n",
        "def get_assistant_response(user_message, llm_chat_history, update_chat_history=True, model_name = \"gpt-3.5-turbo\"):\n",
        "\n",
        "    ## completions as normal\n",
        "    completion = client.chat.completions.create(\n",
        "        model=model_name,\n",
        "        messages=llm_chat_history + [{'role': 'user', 'content': user_message}]\n",
        "    )\n",
        "\n",
        "    ## update chat history if desired\n",
        "    if update_chat_history:\n",
        "        new_messages = [{'role':'user', 'content': user_message},\n",
        "                        {'role': 'assistant', 'content': completion.choices[0].message.content}]\n",
        "        llm_chat_history.extend(new_messages)\n",
        "\n",
        "    ## return the response and updated chat history\n",
        "    return completion.choices[0].message.content, llm_chat_history"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7P3lqA-jYDQ1"
      },
      "source": [
        "The code below demonstrates our initialization and our \"user interface\". Best practice is to separate functionality from appearance."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "BDB4RWQIYDQ1"
      },
      "outputs": [],
      "source": [
        "# create initial chat history\n",
        "system_message = 'You are a helpful assistant. Be brief, succinct, and clear in your responses. Only answer what is asked.'\n",
        "openai_chat_history = [{'role': 'system', 'content': system_message}]\n",
        "\n",
        "# wait for first input\n",
        "print('Begin chatting with the LLM!')\n",
        "user_input = input(\"You: \")\n",
        "\n",
        "# continue chatting until user types 'exit'\n",
        "while user_input != \"exit\":\n",
        "    print('User: ', user_input)\n",
        "    response, openai_chat_history = get_assistant_response(user_input, openai_chat_history)\n",
        "    print(\"Assistant:\", response)\n",
        "    user_input = input(\"You: \")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PM9BTZqwYDQ1"
      },
      "source": [
        "### UI Interaction\n",
        "We can also other programming elements (classes) to help us maintain the state, while helping us format the conversation for use in a streamlined library for developing UIs (gradio)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "McgatKe3OLn3"
      },
      "outputs": [],
      "source": [
        "class OpenAIChatClient:\n",
        "    def __init__(self, model=\"gpt-3.5-turbo\", system_message=\"You are a helpful assistant.\"):\n",
        "        self.client = OpenAI() # assumes API key is in an environment\n",
        "        self.model = model\n",
        "\n",
        "        self.system_message = system_message\n",
        "        self.messages = [{\"role\": \"system\", \"content\": self.system_message}]  # Message list for OpenAI format\n",
        "        self.conversations = []  # Message list for gradio format\n",
        "\n",
        "\n",
        "    def add_user_message(self, text):\n",
        "        self.messages.append({\"role\": \"user\", \"content\": text})\n",
        "        return self.call_completions_api()\n",
        "\n",
        "    def call_completions_api(self):\n",
        "\n",
        "      response = self.client.chat.completions.create(\n",
        "          model=self.model,\n",
        "          messages=self.messages\n",
        "      )\n",
        "\n",
        "      assistant_response = response.choices[0].message.content\n",
        "      self.messages.append({\"role\": \"assistant\", \"content\": assistant_response})\n",
        "      user_message = self.messages[-2]['content']\n",
        "      self.conversations.append((user_message, assistant_response))\n",
        "\n",
        "      return assistant_response\n",
        "\n",
        "    def get_conversation(self):\n",
        "        return self.messages\n",
        "\n",
        "    def get_formatted_conversations(self):\n",
        "        return self.conversations\n",
        "\n",
        "    def pretty_print_conversation(self):\n",
        "        return '\\n'.join([f\"{message['role'].capitalize()}: {message['content']}\" for message in self.messages])\n",
        "\n",
        "    def reset_conversation(self, system_message = None):\n",
        "\n",
        "        # add a new system message if provided\n",
        "        if system_message:\n",
        "            self.system_message = system_message\n",
        "\n",
        "        # reset the messages and conversations\n",
        "        self.messages = [{\"role\": \"system\", \"content\": self.system_message}]\n",
        "        self.conversations = []"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1MKoRBqFYDQ3"
      },
      "source": [
        "We will now explore the behavior without the UI, and then with the UI."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "z8jSKzjs8heq"
      },
      "outputs": [],
      "source": [
        "# create OpenAI Client with history management\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "X1o9bIDZYDQ-"
      },
      "outputs": [],
      "source": [
        "# Simulate first interaction\n",
        "\n",
        "\n",
        "# Do some formatting to make the printing prettier\n",
        "print(custom_chat_client.pretty_print_conversation())\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "omxU2NYV4MWs"
      },
      "outputs": [],
      "source": [
        "# Simulate first interaction\n",
        "_ = custom_chat_client.add_user_message(\"Create a minimal working user interface with the Blocks implementation (not Interface) in Python.\")\n",
        "\n",
        "# Do some formatting to make the printing prettier\n",
        "print(custom_chat_client.pretty_print_conversation())"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gCcVC5I9808K"
      },
      "source": [
        "#### Integrate with gradio\n",
        "Gradio is another platform where the API is new and can be confusing to generative AI. Here, I usually just copy/paste/adapt what I need from the API reference.\n",
        "\n",
        "[Example ChatBot](https://www.gradio.app/guides/creating-a-chatbot-fast)\n",
        "\n",
        "Basic story:\n",
        "* Wherever you see `chatbot_history`, that basically represents message history.\n",
        "* Message history is formatted as a list of `(user_message, bot_message)` tuples. Hence, why we wanted the format above added."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "KFZTGMse88V5"
      },
      "outputs": [],
      "source": [
        "# reset conversation\n",
        "custom_chat_client.reset_conversation()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "cOLRx6L7RWlR"
      },
      "outputs": [],
      "source": [
        "import gradio as gr\n",
        "\n",
        "def respond(message, chat_history):\n",
        "  custom_chat_client.add_user_message(message)\n",
        "  return '', custom_chat_client.get_formatted_conversations()\n",
        "\n",
        "with gr.Blocks() as demo:\n",
        "    chatbot_history = gr.Chatbot()\n",
        "    msg_textbox = gr.Textbox()\n",
        "    reset_button = gr.ClearButton([msg_textbox, chatbot_history]) #doesn't do anything right now\n",
        "\n",
        "    msg_textbox.submit(respond, inputs=[msg_textbox, chatbot_history], outputs=[msg_textbox, chatbot_history])\n",
        "\n",
        "demo.launch()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ue9lZGMFDUVp"
      },
      "source": [
        "# Homework\n",
        "Congratulations! You've made it through an incredible firehose introduction to implementing LLMs programmatically! For homework, you're tasked with gaining more depth into several of the concepts.\n",
        "\n",
        "## Required Exercises\n",
        "### Learning more about LLM Platforms\n",
        "Read and summarize the following short articles to gain more background on the OpenAI API:\n",
        "* [OpenAI API Documentation Introduction](https://platform.openai.com/docs/introduction)\n",
        "* [Text Generation](https://platform.openai.com/docs/guides/text-generation) (Note: You can skip `Completions API (Legacy)`)\n",
        "* [Moderation](https://platform.openai.com/docs/guides/moderation/overview)\n",
        "\n",
        "### Practice with the OpenAI API using Prompt Engineering\n",
        "* Read this short section on [Prompt Engineering by AWS](https://catalog.us-east-1.prod.workshops.aws/workshops/a4bdb007-5600-4368-81c5-ff5b4154f518/en-US/050-prompt-engineering), focusing **ONLY** on the prompt patterns introduced.\n",
        "* Use the OpenAI API to implement each of the patterns. Hint: This will look like the `client.chat.completions.create` cells that we have above, but with modified user messages.\n",
        "* How might some of these patterns benefit you in your project efforts?\n",
        "\n",
        "### Testing Your Understanding of LLM Concepts using a different ChatLLM API\n",
        "#### Implement the 3-turn conversation with Google's Gemini API\n",
        "Now, you will really test your mettle by implementing the 3-turn conversation at the beginning of this notebook using a completely different API - Google's Gemini API. You want to make sure to implement it as \"multi-turn\"; in other words, that you are making sure that the chat history is sent in the full context somehow. There will be differences in this API from the OpenAI implementation. Based on all of the steps above, starting from authentication, implement interaction with the prompts above. Below are some hints/steps to help if needed.\n",
        "\n",
        "<details>\n",
        "<summary>  Show Hints </summary>\n",
        "<div style=\"margin-left: 20px;\">\n",
        "    <details>\n",
        "        <summary>Step 1</summary>\n",
        "        <p>Locate the Gemini API. You can find the platform overview <a href=https://ai.google.dev/>here</a>, or the API documentation <a href=https://ai.google.dev/gemini-api/docs>here</a></p>\n",
        "    </details>\n",
        "    <details>\n",
        "        <summary>Step 2</summary>\n",
        "        <p><strong> Create your API key.</strong> Read the <a href=https://ai.google.dev/gemini-api/docs/quickstart>Quick Start</a>. Note that this Quick Start also has a button where you can \"Run in Google Colab\". That's a great place to start. You can either follow the instructions in the Quick Start, or open the Colab notebook and follow the instructions from there.</p>\n",
        "    </details>\n",
        "    <details>\n",
        "        <summary>Step 3</summary>\n",
        "        <p> <strong> Setup your Colab environment with API keys. </strong> If you haven't already, open up the Colab notebook in the <a href=https://ai.google.dev/gemini-api/docs/quickstart>Quick Start</a> or create your own Colab notebook and copying [while understanding] the cells). Don't forget to use the Colab key in the sidebar. That's where you can set your API key.</p>\n",
        "    </details>\n",
        "    <details>\n",
        "        <summary>Step 4</summary>\n",
        "        <p> <strong> Read the API overview and implement the steps, paying particular attention to the multi-turn conversation. </strong> Sidebars are your friend. Find the API Overview and begin reading to learn more and figure out what you need to do.</p>\n",
        "    </details>\n",
        "    <details>\n",
        "        <summary>Step 5</summary>\n",
        "        <p> Once you understand what you're doing, write/adapt the code so that it does what you expect. Verify the behavior.</p>\n",
        "    </details>\n",
        "    <details>\n",
        "        <summary>Hint</summary>\n",
        "        <p> If all fails, you always have chatGPT/Gemini/ChatLLMofChoice to help you! You have the code on the Gemini API page - use it in your context and ask questions about it to help you come to the answer.</p>\n",
        "    </details>\n",
        "</div>\n",
        "</details>\n",
        "\n",
        "#### Explore Differences\n",
        "Now that you've implemented the functionality, describe the similarities/differences that you see in:\n",
        "* API Key usage\n",
        "* API implementation code structure (e.g., OpenAI used `client.chat.completions.create` - what about Google?)\n",
        "* Model selection\n",
        "* Usage of role-based prompts (i.e., \"user\", \"system\", etc)\n",
        "* Implementation of chat history\n",
        "* Overall programmatic feel\n",
        "* Anything else you observe\n",
        "\n",
        "## Optional Exercises\n",
        "1. We briefly went over applications/interaction using the OpenAI API. Use ChatGPT/Colab AI/ChatLLMofChoice to make sure that you understand the code.\n",
        "2. Ignore the class and function that were created for interaction. Use generative AI (or not) to help you create your own code to maintain chat history when needed.\n",
        "3. Read more about [Gradio](https://www.gradio.app/) (you know what to do! Quickstart -> API Docs) and add other components to the user interface. You can start by using the `reset_button` and adding functionality to reset the chat LLM so it is just the default system message."
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.8"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}