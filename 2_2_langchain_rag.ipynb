{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/zxie52/AI_workshop_2024Summer/blob/main/2_2_langchain_rag.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "a2a28162-8f3e-42f8-83f6-1b9ef38cd394",
      "metadata": {
        "id": "a2a28162-8f3e-42f8-83f6-1b9ef38cd394"
      },
      "source": [
        "<a target=\"_blank\" href=\"https://colab.research.google.com/github/vanderbilt-data-science/ai_summer/blob/main/2_2-langchain-rag.ipynb\">\n",
        "  <img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/>\n",
        "</a>\n",
        "\n",
        "# AI Solutions with Langchain and RAG\n",
        "> For Vanderbilt University AI Summer 2024<br>Prepared by Dr. Charreau Bell\n",
        "\n",
        "_Code versions applicable: May 14, 2024_\n",
        "\n",
        "## Learning Outcomes:\n",
        "* Participants will be able to articulate the essential steps and components of a retrieval-augmented generation (RAG) system and implement a standard RAG system using langchain.\n",
        "* Participants will gain familiarity in inspecting the execution pathways of LLM-based systems.\n",
        "* Participants will gain familiarity in approaches for the evaluation of LLM-based systems."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "8c1cebcf-1ac1-48aa-b998-9f0b2a77567f",
      "metadata": {
        "id": "8c1cebcf-1ac1-48aa-b998-9f0b2a77567f"
      },
      "source": [
        "### Computing Environment Setup"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "15202e4f-5a92-4231-a557-43daa913beb5",
      "metadata": {
        "id": "15202e4f-5a92-4231-a557-43daa913beb5",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "58057a85-7de3-4002-cb4e-d32f6f9eef7f"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting langchain==0.1.20\n",
            "  Downloading langchain-0.1.20-py3-none-any.whl (1.0 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.0/1.0 MB\u001b[0m \u001b[31m6.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting langchain_openai\n",
            "  Downloading langchain_openai-0.1.6-py3-none-any.whl (34 kB)\n",
            "Collecting grandalf\n",
            "  Downloading grandalf-0.8-py3-none-any.whl (41 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m41.8/41.8 kB\u001b[0m \u001b[31m4.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting sentence-transformers\n",
            "  Downloading sentence_transformers-2.7.0-py3-none-any.whl (171 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m171.5/171.5 kB\u001b[0m \u001b[31m7.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: PyYAML>=5.3 in /usr/local/lib/python3.10/dist-packages (from langchain==0.1.20) (6.0.1)\n",
            "Requirement already satisfied: SQLAlchemy<3,>=1.4 in /usr/local/lib/python3.10/dist-packages (from langchain==0.1.20) (2.0.30)\n",
            "Requirement already satisfied: aiohttp<4.0.0,>=3.8.3 in /usr/local/lib/python3.10/dist-packages (from langchain==0.1.20) (3.9.5)\n",
            "Requirement already satisfied: async-timeout<5.0.0,>=4.0.0 in /usr/local/lib/python3.10/dist-packages (from langchain==0.1.20) (4.0.3)\n",
            "Collecting dataclasses-json<0.7,>=0.5.7 (from langchain==0.1.20)\n",
            "  Downloading dataclasses_json-0.6.6-py3-none-any.whl (28 kB)\n",
            "Collecting langchain-community<0.1,>=0.0.38 (from langchain==0.1.20)\n",
            "  Downloading langchain_community-0.0.38-py3-none-any.whl (2.0 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.0/2.0 MB\u001b[0m \u001b[31m11.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting langchain-core<0.2.0,>=0.1.52 (from langchain==0.1.20)\n",
            "  Downloading langchain_core-0.1.52-py3-none-any.whl (302 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m302.9/302.9 kB\u001b[0m \u001b[31m10.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting langchain-text-splitters<0.1,>=0.0.1 (from langchain==0.1.20)\n",
            "  Downloading langchain_text_splitters-0.0.1-py3-none-any.whl (21 kB)\n",
            "Collecting langsmith<0.2.0,>=0.1.17 (from langchain==0.1.20)\n",
            "  Downloading langsmith-0.1.57-py3-none-any.whl (121 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m121.0/121.0 kB\u001b[0m \u001b[31m4.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: numpy<2,>=1 in /usr/local/lib/python3.10/dist-packages (from langchain==0.1.20) (1.25.2)\n",
            "Requirement already satisfied: pydantic<3,>=1 in /usr/local/lib/python3.10/dist-packages (from langchain==0.1.20) (2.7.1)\n",
            "Requirement already satisfied: requests<3,>=2 in /usr/local/lib/python3.10/dist-packages (from langchain==0.1.20) (2.31.0)\n",
            "Requirement already satisfied: tenacity<9.0.0,>=8.1.0 in /usr/local/lib/python3.10/dist-packages (from langchain==0.1.20) (8.3.0)\n",
            "Collecting openai<2.0.0,>=1.24.0 (from langchain_openai)\n",
            "  Downloading openai-1.30.1-py3-none-any.whl (320 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m320.6/320.6 kB\u001b[0m \u001b[31m12.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting tiktoken<1,>=0.5.2 (from langchain_openai)\n",
            "  Downloading tiktoken-0.7.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.1 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.1/1.1 MB\u001b[0m \u001b[31m10.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: pyparsing in /usr/local/lib/python3.10/dist-packages (from grandalf) (3.1.2)\n",
            "Requirement already satisfied: transformers<5.0.0,>=4.34.0 in /usr/local/lib/python3.10/dist-packages (from sentence-transformers) (4.40.2)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (from sentence-transformers) (4.66.4)\n",
            "Requirement already satisfied: torch>=1.11.0 in /usr/local/lib/python3.10/dist-packages (from sentence-transformers) (2.2.1+cu121)\n",
            "Requirement already satisfied: scikit-learn in /usr/local/lib/python3.10/dist-packages (from sentence-transformers) (1.2.2)\n",
            "Requirement already satisfied: scipy in /usr/local/lib/python3.10/dist-packages (from sentence-transformers) (1.11.4)\n",
            "Requirement already satisfied: huggingface-hub>=0.15.1 in /usr/local/lib/python3.10/dist-packages (from sentence-transformers) (0.20.3)\n",
            "Requirement already satisfied: Pillow in /usr/local/lib/python3.10/dist-packages (from sentence-transformers) (9.4.0)\n",
            "Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain==0.1.20) (1.3.1)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain==0.1.20) (23.2.0)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain==0.1.20) (1.4.1)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain==0.1.20) (6.0.5)\n",
            "Requirement already satisfied: yarl<2.0,>=1.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain==0.1.20) (1.9.4)\n",
            "Collecting marshmallow<4.0.0,>=3.18.0 (from dataclasses-json<0.7,>=0.5.7->langchain==0.1.20)\n",
            "  Downloading marshmallow-3.21.2-py3-none-any.whl (49 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m49.3/49.3 kB\u001b[0m \u001b[31m1.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting typing-inspect<1,>=0.4.0 (from dataclasses-json<0.7,>=0.5.7->langchain==0.1.20)\n",
            "  Downloading typing_inspect-0.9.0-py3-none-any.whl (8.8 kB)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from huggingface-hub>=0.15.1->sentence-transformers) (3.14.0)\n",
            "Requirement already satisfied: fsspec>=2023.5.0 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub>=0.15.1->sentence-transformers) (2023.6.0)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub>=0.15.1->sentence-transformers) (4.11.0)\n",
            "Requirement already satisfied: packaging>=20.9 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub>=0.15.1->sentence-transformers) (24.0)\n",
            "Collecting jsonpatch<2.0,>=1.33 (from langchain-core<0.2.0,>=0.1.52->langchain==0.1.20)\n",
            "  Downloading jsonpatch-1.33-py2.py3-none-any.whl (12 kB)\n",
            "Collecting packaging>=20.9 (from huggingface-hub>=0.15.1->sentence-transformers)\n",
            "  Downloading packaging-23.2-py3-none-any.whl (53 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m53.0/53.0 kB\u001b[0m \u001b[31m4.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting orjson<4.0.0,>=3.9.14 (from langsmith<0.2.0,>=0.1.17->langchain==0.1.20)\n",
            "  Downloading orjson-3.10.3-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (142 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m142.5/142.5 kB\u001b[0m \u001b[31m2.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: anyio<5,>=3.5.0 in /usr/local/lib/python3.10/dist-packages (from openai<2.0.0,>=1.24.0->langchain_openai) (3.7.1)\n",
            "Requirement already satisfied: distro<2,>=1.7.0 in /usr/lib/python3/dist-packages (from openai<2.0.0,>=1.24.0->langchain_openai) (1.7.0)\n",
            "Collecting httpx<1,>=0.23.0 (from openai<2.0.0,>=1.24.0->langchain_openai)\n",
            "  Downloading httpx-0.27.0-py3-none-any.whl (75 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m75.6/75.6 kB\u001b[0m \u001b[31m4.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: sniffio in /usr/local/lib/python3.10/dist-packages (from openai<2.0.0,>=1.24.0->langchain_openai) (1.3.1)\n",
            "Requirement already satisfied: annotated-types>=0.4.0 in /usr/local/lib/python3.10/dist-packages (from pydantic<3,>=1->langchain==0.1.20) (0.6.0)\n",
            "Requirement already satisfied: pydantic-core==2.18.2 in /usr/local/lib/python3.10/dist-packages (from pydantic<3,>=1->langchain==0.1.20) (2.18.2)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2->langchain==0.1.20) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2->langchain==0.1.20) (3.7)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2->langchain==0.1.20) (2.0.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2->langchain==0.1.20) (2024.2.2)\n",
            "Requirement already satisfied: greenlet!=0.4.17 in /usr/local/lib/python3.10/dist-packages (from SQLAlchemy<3,>=1.4->langchain==0.1.20) (3.0.3)\n",
            "Requirement already satisfied: regex>=2022.1.18 in /usr/local/lib/python3.10/dist-packages (from tiktoken<1,>=0.5.2->langchain_openai) (2023.12.25)\n",
            "Requirement already satisfied: sympy in /usr/local/lib/python3.10/dist-packages (from torch>=1.11.0->sentence-transformers) (1.12)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch>=1.11.0->sentence-transformers) (3.3)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch>=1.11.0->sentence-transformers) (3.1.4)\n",
            "Collecting nvidia-cuda-nvrtc-cu12==12.1.105 (from torch>=1.11.0->sentence-transformers)\n",
            "  Using cached nvidia_cuda_nvrtc_cu12-12.1.105-py3-none-manylinux1_x86_64.whl (23.7 MB)\n",
            "Collecting nvidia-cuda-runtime-cu12==12.1.105 (from torch>=1.11.0->sentence-transformers)\n",
            "  Using cached nvidia_cuda_runtime_cu12-12.1.105-py3-none-manylinux1_x86_64.whl (823 kB)\n",
            "Collecting nvidia-cuda-cupti-cu12==12.1.105 (from torch>=1.11.0->sentence-transformers)\n",
            "  Using cached nvidia_cuda_cupti_cu12-12.1.105-py3-none-manylinux1_x86_64.whl (14.1 MB)\n",
            "Collecting nvidia-cudnn-cu12==8.9.2.26 (from torch>=1.11.0->sentence-transformers)\n",
            "  Using cached nvidia_cudnn_cu12-8.9.2.26-py3-none-manylinux1_x86_64.whl (731.7 MB)\n",
            "Collecting nvidia-cublas-cu12==12.1.3.1 (from torch>=1.11.0->sentence-transformers)\n",
            "  Using cached nvidia_cublas_cu12-12.1.3.1-py3-none-manylinux1_x86_64.whl (410.6 MB)\n",
            "Collecting nvidia-cufft-cu12==11.0.2.54 (from torch>=1.11.0->sentence-transformers)\n",
            "  Using cached nvidia_cufft_cu12-11.0.2.54-py3-none-manylinux1_x86_64.whl (121.6 MB)\n",
            "Collecting nvidia-curand-cu12==10.3.2.106 (from torch>=1.11.0->sentence-transformers)\n",
            "  Using cached nvidia_curand_cu12-10.3.2.106-py3-none-manylinux1_x86_64.whl (56.5 MB)\n",
            "Collecting nvidia-cusolver-cu12==11.4.5.107 (from torch>=1.11.0->sentence-transformers)\n",
            "  Using cached nvidia_cusolver_cu12-11.4.5.107-py3-none-manylinux1_x86_64.whl (124.2 MB)\n",
            "Collecting nvidia-cusparse-cu12==12.1.0.106 (from torch>=1.11.0->sentence-transformers)\n",
            "  Using cached nvidia_cusparse_cu12-12.1.0.106-py3-none-manylinux1_x86_64.whl (196.0 MB)\n",
            "Collecting nvidia-nccl-cu12==2.19.3 (from torch>=1.11.0->sentence-transformers)\n",
            "  Using cached nvidia_nccl_cu12-2.19.3-py3-none-manylinux1_x86_64.whl (166.0 MB)\n",
            "Collecting nvidia-nvtx-cu12==12.1.105 (from torch>=1.11.0->sentence-transformers)\n",
            "  Using cached nvidia_nvtx_cu12-12.1.105-py3-none-manylinux1_x86_64.whl (99 kB)\n",
            "Requirement already satisfied: triton==2.2.0 in /usr/local/lib/python3.10/dist-packages (from torch>=1.11.0->sentence-transformers) (2.2.0)\n",
            "Collecting nvidia-nvjitlink-cu12 (from nvidia-cusolver-cu12==11.4.5.107->torch>=1.11.0->sentence-transformers)\n",
            "  Using cached nvidia_nvjitlink_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (21.1 MB)\n",
            "Requirement already satisfied: tokenizers<0.20,>=0.19 in /usr/local/lib/python3.10/dist-packages (from transformers<5.0.0,>=4.34.0->sentence-transformers) (0.19.1)\n",
            "Requirement already satisfied: safetensors>=0.4.1 in /usr/local/lib/python3.10/dist-packages (from transformers<5.0.0,>=4.34.0->sentence-transformers) (0.4.3)\n",
            "Requirement already satisfied: joblib>=1.1.1 in /usr/local/lib/python3.10/dist-packages (from scikit-learn->sentence-transformers) (1.4.2)\n",
            "Requirement already satisfied: threadpoolctl>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from scikit-learn->sentence-transformers) (3.5.0)\n",
            "Requirement already satisfied: exceptiongroup in /usr/local/lib/python3.10/dist-packages (from anyio<5,>=3.5.0->openai<2.0.0,>=1.24.0->langchain_openai) (1.2.1)\n",
            "Collecting httpcore==1.* (from httpx<1,>=0.23.0->openai<2.0.0,>=1.24.0->langchain_openai)\n",
            "  Downloading httpcore-1.0.5-py3-none-any.whl (77 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m77.9/77.9 kB\u001b[0m \u001b[31m10.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting h11<0.15,>=0.13 (from httpcore==1.*->httpx<1,>=0.23.0->openai<2.0.0,>=1.24.0->langchain_openai)\n",
            "  Downloading h11-0.14.0-py3-none-any.whl (58 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m58.3/58.3 kB\u001b[0m \u001b[31m6.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting jsonpointer>=1.9 (from jsonpatch<2.0,>=1.33->langchain-core<0.2.0,>=0.1.52->langchain==0.1.20)\n",
            "  Downloading jsonpointer-2.4-py2.py3-none-any.whl (7.8 kB)\n",
            "Collecting mypy-extensions>=0.3.0 (from typing-inspect<1,>=0.4.0->dataclasses-json<0.7,>=0.5.7->langchain==0.1.20)\n",
            "  Downloading mypy_extensions-1.0.0-py3-none-any.whl (4.7 kB)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch>=1.11.0->sentence-transformers) (2.1.5)\n",
            "Requirement already satisfied: mpmath>=0.19 in /usr/local/lib/python3.10/dist-packages (from sympy->torch>=1.11.0->sentence-transformers) (1.3.0)\n",
            "Installing collected packages: packaging, orjson, nvidia-nvtx-cu12, nvidia-nvjitlink-cu12, nvidia-nccl-cu12, nvidia-curand-cu12, nvidia-cufft-cu12, nvidia-cuda-runtime-cu12, nvidia-cuda-nvrtc-cu12, nvidia-cuda-cupti-cu12, nvidia-cublas-cu12, mypy-extensions, jsonpointer, h11, grandalf, typing-inspect, tiktoken, nvidia-cusparse-cu12, nvidia-cudnn-cu12, marshmallow, jsonpatch, httpcore, nvidia-cusolver-cu12, langsmith, httpx, dataclasses-json, openai, langchain-core, sentence-transformers, langchain-text-splitters, langchain_openai, langchain-community, langchain\n",
            "  Attempting uninstall: packaging\n",
            "    Found existing installation: packaging 24.0\n",
            "    Uninstalling packaging-24.0:\n",
            "      Successfully uninstalled packaging-24.0\n",
            "Successfully installed dataclasses-json-0.6.6 grandalf-0.8 h11-0.14.0 httpcore-1.0.5 httpx-0.27.0 jsonpatch-1.33 jsonpointer-2.4 langchain-0.1.20 langchain-community-0.0.38 langchain-core-0.1.52 langchain-text-splitters-0.0.1 langchain_openai-0.1.6 langsmith-0.1.57 marshmallow-3.21.2 mypy-extensions-1.0.0 nvidia-cublas-cu12-12.1.3.1 nvidia-cuda-cupti-cu12-12.1.105 nvidia-cuda-nvrtc-cu12-12.1.105 nvidia-cuda-runtime-cu12-12.1.105 nvidia-cudnn-cu12-8.9.2.26 nvidia-cufft-cu12-11.0.2.54 nvidia-curand-cu12-10.3.2.106 nvidia-cusolver-cu12-11.4.5.107 nvidia-cusparse-cu12-12.1.0.106 nvidia-nccl-cu12-2.19.3 nvidia-nvjitlink-cu12-12.4.127 nvidia-nvtx-cu12-12.1.105 openai-1.30.1 orjson-3.10.3 packaging-23.2 sentence-transformers-2.7.0 tiktoken-0.7.0 typing-inspect-0.9.0\n",
            "Collecting pypdf\n",
            "  Downloading pypdf-4.2.0-py3-none-any.whl (290 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m290.4/290.4 kB\u001b[0m \u001b[31m4.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting chromadb\n",
            "  Downloading chromadb-0.5.0-py3-none-any.whl (526 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m526.8/526.8 kB\u001b[0m \u001b[31m9.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting faiss-cpu\n",
            "  Downloading faiss_cpu-1.8.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (27.0 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m27.0/27.0 MB\u001b[0m \u001b[31m36.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: typing_extensions>=4.0 in /usr/local/lib/python3.10/dist-packages (from pypdf) (4.11.0)\n",
            "Requirement already satisfied: build>=1.0.3 in /usr/local/lib/python3.10/dist-packages (from chromadb) (1.2.1)\n",
            "Requirement already satisfied: requests>=2.28 in /usr/local/lib/python3.10/dist-packages (from chromadb) (2.31.0)\n",
            "Requirement already satisfied: pydantic>=1.9 in /usr/local/lib/python3.10/dist-packages (from chromadb) (2.7.1)\n",
            "Collecting chroma-hnswlib==0.7.3 (from chromadb)\n",
            "  Downloading chroma_hnswlib-0.7.3-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (2.4 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.4/2.4 MB\u001b[0m \u001b[31m65.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting fastapi>=0.95.2 (from chromadb)\n",
            "  Downloading fastapi-0.111.0-py3-none-any.whl (91 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m92.0/92.0 kB\u001b[0m \u001b[31m12.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting uvicorn[standard]>=0.18.3 (from chromadb)\n",
            "  Downloading uvicorn-0.29.0-py3-none-any.whl (60 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m60.8/60.8 kB\u001b[0m \u001b[31m8.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: numpy>=1.22.5 in /usr/local/lib/python3.10/dist-packages (from chromadb) (1.25.2)\n",
            "Collecting posthog>=2.4.0 (from chromadb)\n",
            "  Downloading posthog-3.5.0-py2.py3-none-any.whl (41 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m41.3/41.3 kB\u001b[0m \u001b[31m4.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting onnxruntime>=1.14.1 (from chromadb)\n",
            "  Downloading onnxruntime-1.17.3-cp310-cp310-manylinux_2_27_x86_64.manylinux_2_28_x86_64.whl (6.8 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m6.8/6.8 MB\u001b[0m \u001b[31m91.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting opentelemetry-api>=1.2.0 (from chromadb)\n",
            "  Downloading opentelemetry_api-1.24.0-py3-none-any.whl (60 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m60.1/60.1 kB\u001b[0m \u001b[31m9.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting opentelemetry-exporter-otlp-proto-grpc>=1.2.0 (from chromadb)\n",
            "  Downloading opentelemetry_exporter_otlp_proto_grpc-1.24.0-py3-none-any.whl (18 kB)\n",
            "Collecting opentelemetry-instrumentation-fastapi>=0.41b0 (from chromadb)\n",
            "  Downloading opentelemetry_instrumentation_fastapi-0.45b0-py3-none-any.whl (11 kB)\n",
            "Collecting opentelemetry-sdk>=1.2.0 (from chromadb)\n",
            "  Downloading opentelemetry_sdk-1.24.0-py3-none-any.whl (106 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m106.1/106.1 kB\u001b[0m \u001b[31m13.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: tokenizers>=0.13.2 in /usr/local/lib/python3.10/dist-packages (from chromadb) (0.19.1)\n",
            "Collecting pypika>=0.48.9 (from chromadb)\n",
            "  Downloading PyPika-0.48.9.tar.gz (67 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m67.3/67.3 kB\u001b[0m \u001b[31m8.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Installing build dependencies ... \u001b[?25l\u001b[?25hdone\n",
            "  Getting requirements to build wheel ... \u001b[?25l\u001b[?25hdone\n",
            "  Preparing metadata (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: tqdm>=4.65.0 in /usr/local/lib/python3.10/dist-packages (from chromadb) (4.66.4)\n",
            "Collecting overrides>=7.3.1 (from chromadb)\n",
            "  Downloading overrides-7.7.0-py3-none-any.whl (17 kB)\n",
            "Requirement already satisfied: importlib-resources in /usr/local/lib/python3.10/dist-packages (from chromadb) (6.4.0)\n",
            "Requirement already satisfied: grpcio>=1.58.0 in /usr/local/lib/python3.10/dist-packages (from chromadb) (1.63.0)\n",
            "Collecting bcrypt>=4.0.1 (from chromadb)\n",
            "  Downloading bcrypt-4.1.3-cp39-abi3-manylinux_2_28_x86_64.whl (283 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m283.7/283.7 kB\u001b[0m \u001b[31m33.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: typer>=0.9.0 in /usr/local/lib/python3.10/dist-packages (from chromadb) (0.9.4)\n",
            "Collecting kubernetes>=28.1.0 (from chromadb)\n",
            "  Downloading kubernetes-29.0.0-py2.py3-none-any.whl (1.6 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.6/1.6 MB\u001b[0m \u001b[31m78.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: tenacity>=8.2.3 in /usr/local/lib/python3.10/dist-packages (from chromadb) (8.3.0)\n",
            "Requirement already satisfied: PyYAML>=6.0.0 in /usr/local/lib/python3.10/dist-packages (from chromadb) (6.0.1)\n",
            "Collecting mmh3>=4.0.1 (from chromadb)\n",
            "  Downloading mmh3-4.1.0-cp310-cp310-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl (67 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m67.6/67.6 kB\u001b[0m \u001b[31m9.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: orjson>=3.9.12 in /usr/local/lib/python3.10/dist-packages (from chromadb) (3.10.3)\n",
            "Requirement already satisfied: packaging>=19.1 in /usr/local/lib/python3.10/dist-packages (from build>=1.0.3->chromadb) (23.2)\n",
            "Requirement already satisfied: pyproject_hooks in /usr/local/lib/python3.10/dist-packages (from build>=1.0.3->chromadb) (1.1.0)\n",
            "Requirement already satisfied: tomli>=1.1.0 in /usr/local/lib/python3.10/dist-packages (from build>=1.0.3->chromadb) (2.0.1)\n",
            "Collecting starlette<0.38.0,>=0.37.2 (from fastapi>=0.95.2->chromadb)\n",
            "  Downloading starlette-0.37.2-py3-none-any.whl (71 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m71.9/71.9 kB\u001b[0m \u001b[31m10.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting fastapi-cli>=0.0.2 (from fastapi>=0.95.2->chromadb)\n",
            "  Downloading fastapi_cli-0.0.3-py3-none-any.whl (9.2 kB)\n",
            "Requirement already satisfied: httpx>=0.23.0 in /usr/local/lib/python3.10/dist-packages (from fastapi>=0.95.2->chromadb) (0.27.0)\n",
            "Requirement already satisfied: jinja2>=2.11.2 in /usr/local/lib/python3.10/dist-packages (from fastapi>=0.95.2->chromadb) (3.1.4)\n",
            "Collecting python-multipart>=0.0.7 (from fastapi>=0.95.2->chromadb)\n",
            "  Downloading python_multipart-0.0.9-py3-none-any.whl (22 kB)\n",
            "Collecting ujson!=4.0.2,!=4.1.0,!=4.2.0,!=4.3.0,!=5.0.0,!=5.1.0,>=4.0.1 (from fastapi>=0.95.2->chromadb)\n",
            "  Downloading ujson-5.10.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (53 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m53.6/53.6 kB\u001b[0m \u001b[31m4.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting email_validator>=2.0.0 (from fastapi>=0.95.2->chromadb)\n",
            "  Downloading email_validator-2.1.1-py3-none-any.whl (30 kB)\n",
            "Requirement already satisfied: certifi>=14.05.14 in /usr/local/lib/python3.10/dist-packages (from kubernetes>=28.1.0->chromadb) (2024.2.2)\n",
            "Requirement already satisfied: six>=1.9.0 in /usr/local/lib/python3.10/dist-packages (from kubernetes>=28.1.0->chromadb) (1.16.0)\n",
            "Requirement already satisfied: python-dateutil>=2.5.3 in /usr/local/lib/python3.10/dist-packages (from kubernetes>=28.1.0->chromadb) (2.8.2)\n",
            "Requirement already satisfied: google-auth>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from kubernetes>=28.1.0->chromadb) (2.27.0)\n",
            "Requirement already satisfied: websocket-client!=0.40.0,!=0.41.*,!=0.42.*,>=0.32.0 in /usr/local/lib/python3.10/dist-packages (from kubernetes>=28.1.0->chromadb) (1.8.0)\n",
            "Requirement already satisfied: requests-oauthlib in /usr/local/lib/python3.10/dist-packages (from kubernetes>=28.1.0->chromadb) (1.3.1)\n",
            "Requirement already satisfied: oauthlib>=3.2.2 in /usr/local/lib/python3.10/dist-packages (from kubernetes>=28.1.0->chromadb) (3.2.2)\n",
            "Requirement already satisfied: urllib3>=1.24.2 in /usr/local/lib/python3.10/dist-packages (from kubernetes>=28.1.0->chromadb) (2.0.7)\n",
            "Collecting coloredlogs (from onnxruntime>=1.14.1->chromadb)\n",
            "  Downloading coloredlogs-15.0.1-py2.py3-none-any.whl (46 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m46.0/46.0 kB\u001b[0m \u001b[31m5.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: flatbuffers in /usr/local/lib/python3.10/dist-packages (from onnxruntime>=1.14.1->chromadb) (24.3.25)\n",
            "Requirement already satisfied: protobuf in /usr/local/lib/python3.10/dist-packages (from onnxruntime>=1.14.1->chromadb) (3.20.3)\n",
            "Requirement already satisfied: sympy in /usr/local/lib/python3.10/dist-packages (from onnxruntime>=1.14.1->chromadb) (1.12)\n",
            "Collecting deprecated>=1.2.6 (from opentelemetry-api>=1.2.0->chromadb)\n",
            "  Downloading Deprecated-1.2.14-py2.py3-none-any.whl (9.6 kB)\n",
            "Collecting importlib-metadata<=7.0,>=6.0 (from opentelemetry-api>=1.2.0->chromadb)\n",
            "  Downloading importlib_metadata-7.0.0-py3-none-any.whl (23 kB)\n",
            "Requirement already satisfied: googleapis-common-protos~=1.52 in /usr/local/lib/python3.10/dist-packages (from opentelemetry-exporter-otlp-proto-grpc>=1.2.0->chromadb) (1.63.0)\n"
          ]
        }
      ],
      "source": [
        "! pip install langchain==0.1.20 langchain_openai grandalf sentence-transformers\n",
        "! pip install pypdf chromadb faiss-cpu"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "dd8f69f2",
      "metadata": {
        "id": "dd8f69f2"
      },
      "outputs": [],
      "source": [
        "# Best practice is to do all imports at the beginning of the notebook, but we have separated them here for learning purposes.\n",
        "import os"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "e0e06178",
      "metadata": {
        "id": "e0e06178"
      },
      "outputs": [],
      "source": [
        "# auth replicated here for reference just in case you choose to do something similar\n",
        "from google.colab import userdata\n",
        "os.environ['OPENAI_API_KEY'] = userdata.get('OPENAI_API_KEY')"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "88934c0a-c340-4678-be23-563a4fe331f1",
      "metadata": {
        "id": "88934c0a-c340-4678-be23-563a4fe331f1"
      },
      "source": [
        "## Langchain Introduction\n",
        "\n",
        "### Overview of System\n",
        "\n",
        "[Overview of Langchain](https://python.langchain.com/v0.1/docs/get_started/introduction/)\n",
        "\n",
        "<figure>\n",
        "<img src='https://python.langchain.com/v0.1/svg/langchain_stack.svg' height=600/>\n",
        "    <figcaption>\n",
        "        Langchain Overview, from <a href=https://python.langchain.com/v0.1/docs/get_started/introduction>Langchain Introduction</a>\n",
        "    </figcaption>\n",
        "</figure>\n",
        "\n",
        "### Quick Start\n",
        "To, as it says - start quickly - get started using the [Quick Start](https://python.langchain.com/v0.1/docs/get_started/quickstart/) page.\n",
        "\n",
        "### Details of Individual Composition Components\n",
        "To learn more about any of the individual components used below, use the [Components Page](https://python.langchain.com/v0.1/docs/modules/)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "477bff1b",
      "metadata": {
        "id": "477bff1b"
      },
      "source": [
        "## Review of python formatted strings\n",
        "To prepare ourselves for langchain, we'll first review formatted strings."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "2cff2bcd",
      "metadata": {
        "id": "2cff2bcd"
      },
      "outputs": [],
      "source": [
        "# basic functionality of print\n",
        "print('Tell me a story about cats')\n",
        "\n",
        "# with variables\n",
        "prompt_string = 'Tell me a story about cats'\n",
        "print('As string ', prompt_string)\n",
        "\n",
        "# as formatted string\n",
        "prompt_string = 'Tell me a story about cats'\n",
        "print(f\"With formatted string: {prompt_string}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "d708c91c",
      "metadata": {
        "id": "d708c91c"
      },
      "source": [
        "Motivating example: you are building a GPT that tells stories. The user just needs to provide the topic."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "afd22772",
      "metadata": {
        "id": "afd22772"
      },
      "outputs": [],
      "source": [
        "# as a template string\n",
        "string_prompt_template = f\"Tell me a story about {{topic}}\"\n",
        "string_prompt_template"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "70e55c40",
      "metadata": {
        "id": "70e55c40"
      },
      "outputs": [],
      "source": [
        "# you can fill in the template at a later time\n",
        "string_prompt_template.format(topic='cats')"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "6a7ffaf4",
      "metadata": {
        "id": "6a7ffaf4"
      },
      "source": [
        "## Langchain Prompt Templates\n",
        "> Formatting and arranging prompt strings\n",
        "\n",
        "Langchain prompt templates work just like this, but with additional functionality targeted towards LLM interaction. There are lots of different prompt templates, but here, we'll focus on two: `PromptTemplate`, and `ChatPromptTemplate`.\n",
        "\n",
        "**Additional resources**: [Guide on Prompt Templates](https://python.langchain.com/v0.1/docs/modules/model_io/prompts/quick_start/)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "71f3192e",
      "metadata": {
        "id": "71f3192e"
      },
      "outputs": [],
      "source": [
        "from langchain_core.prompts import PromptTemplate, ChatPromptTemplate, MessagesPlaceholder\n",
        "from langchain_core.output_parsers import StrOutputParser"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "ef3a0cbf",
      "metadata": {
        "id": "ef3a0cbf"
      },
      "outputs": [],
      "source": [
        "# create system messsage for shorter responses\n",
        "brief_system_message = 'You are a helpful assistant. Be brief, succinct, and clear in your responses. Only answer what is asked.'"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "f581ae79",
      "metadata": {
        "id": "f581ae79"
      },
      "source": [
        "### PromptTemplate"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "4dda49b6",
      "metadata": {
        "id": "4dda49b6"
      },
      "outputs": [],
      "source": [
        "# create a langchain prompt template\n",
        "lc_prompt\n",
        "\n",
        "# has standard behavior of f-strings\n",
        "lc_prompt\n",
        "\n",
        "# but also has additional functionality\n",
        "lc_prompt"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "38033eec",
      "metadata": {
        "id": "38033eec"
      },
      "source": [
        "### ChatPromptTemplate"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "f253a330",
      "metadata": {
        "id": "f253a330"
      },
      "outputs": [],
      "source": [
        "# create prompt template\n",
        "lc_chat_prompt_template = ChatPromptTemplate.from_template(\"tell me a story about {topic}\")\n",
        "\n",
        "# has invocation functionality resulting to chat-style messages\n",
        "lc_chat_prompt_template.invoke({'topic':'cats'})"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "f6896b8a",
      "metadata": {
        "id": "f6896b8a"
      },
      "outputs": [],
      "source": [
        "# create message-based chat prompt template\n",
        "lc_chat_prompt_template = ChatPromptTemplate.from_messages(\n",
        "    # create messages similar to OpenAI API\n",
        ")\n",
        "\n",
        "# invoke the chat prompt template\n",
        "lc_chat_prompt_template.invoke({'topic':'cats'})"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "20703088",
      "metadata": {
        "id": "20703088"
      },
      "source": [
        "## Langchain Expression Language (LCEL)\n",
        "**Resource:** [LCEL Overview](https://python.langchain.com/v0.1/docs/expression_language/)\n",
        "Main Points:\n",
        "* Runnable Protocol\n",
        "* Known inputs and outputs on invoke\n",
        "* Flexibility in chain assembly\n",
        "* [Standard Interface](https://python.langchain.com/v0.1/docs/expression_language/interface/)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "2D0GyPPpedXw",
      "metadata": {
        "id": "2D0GyPPpedXw"
      },
      "source": [
        "# Basic Model Chains/ Model I/O\n",
        "\n",
        "**Resource**: [Detailed Guide](https://python.langchain.com/v0.1/docs/modules/)\n",
        "\n",
        "## Basic Prompt/Model Chain\n",
        "See [Prompt+LLM](https://python.langchain.com/docs/expression_language/cookbook/prompt_llm_parser) for more information"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "a5d1b35f",
      "metadata": {
        "id": "a5d1b35f"
      },
      "outputs": [],
      "source": [
        "from langchain_openai import ChatOpenAI"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "4f068a62-9a56-420a-be6f-2bf3bb805d04",
      "metadata": {
        "id": "4f068a62-9a56-420a-be6f-2bf3bb805d04"
      },
      "outputs": [],
      "source": [
        "prompt = ChatPromptTemplate.from_template(\"tell me a joke about {foo}\")\n",
        "model\n",
        "chain"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "1GVTTau0fsdd",
      "metadata": {
        "id": "1GVTTau0fsdd"
      },
      "outputs": [],
      "source": [
        "# Observe what the prompt looks like when we substitute words into it\n",
        "prompt.invoke({'foo':\"cats\"})"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "VP7khR3zd2OJ",
      "metadata": {
        "id": "VP7khR3zd2OJ"
      },
      "outputs": [],
      "source": [
        "# Now, actually call the entire chain on it\n",
        "\n",
        "print(res)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "d753e10d",
      "metadata": {
        "id": "d753e10d"
      },
      "source": [
        "A little helper visualization:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "53dc608a",
      "metadata": {
        "id": "53dc608a"
      },
      "outputs": [],
      "source": [
        "# create visualization\n",
        "chain.get_graph().print_ascii()"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "LUCuzx2HfeVx",
      "metadata": {
        "id": "LUCuzx2HfeVx"
      },
      "source": [
        "## Even more simplified prompt chains"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "eM3wDIpnfg3f",
      "metadata": {
        "id": "eM3wDIpnfg3f"
      },
      "outputs": [],
      "source": [
        "# Create total user prompt chain\n",
        "prompt = ChatPromptTemplate.from_template(\"{text}\")\n",
        "\n",
        "# Add output parser\n",
        "chain = prompt | model | StrOutputParser()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "8hStGS1kfonX",
      "metadata": {
        "id": "8hStGS1kfonX"
      },
      "outputs": [],
      "source": [
        "# Now, the user can submit literally whatever\n",
        "res = chain.invoke({'text':\"Briefly and succintly summarize Episodes 4-6 of Star Wars.\"})\n",
        "print(res)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "958be68e",
      "metadata": {
        "id": "958be68e"
      },
      "source": [
        "## What just happened? Inspecting model behavior\n",
        "Several ways to do this:\n",
        "* `langchain` verbosity/debugging\n",
        "* `langsmith`"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "db9684a3",
      "metadata": {
        "id": "db9684a3"
      },
      "source": [
        "### Langchain\n",
        "Resource: [Guides -> Langchain Debugging](https://python.langchain.com/v0.1/docs/guides/development/debugging/)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "e76bf06e",
      "metadata": {
        "id": "e76bf06e"
      },
      "outputs": [],
      "source": [
        "from langchain.globals import set_debug, set_verbose"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "88edac26",
      "metadata": {
        "id": "88edac26"
      },
      "outputs": [],
      "source": [
        "set_debug(True)\n",
        "set_verbose(True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "1984a1a0",
      "metadata": {
        "id": "1984a1a0"
      },
      "outputs": [],
      "source": [
        "# Basic prompt -> model -> parser chain\n",
        "chain = prompt | model | StrOutputParser()\n",
        "chain.invoke('What is a python f-string?')"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "4e60883c",
      "metadata": {
        "id": "4e60883c"
      },
      "source": [
        "### Langsmith\n",
        "Resource: [Tracing Langchain with Langsmith](https://docs.smith.langchain.com/how_to_guides/tracing/trace_with_langchain)\n",
        "\n",
        "Don't have a langsmith API Key yet? You'll need a user account on [Langsmith](https://smith.langchain.com/). Then, follow these [instructions provided by langsmith](https://docs.smith.langchain.com/#2-create-an-api-key)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "6948026e",
      "metadata": {
        "id": "6948026e"
      },
      "outputs": [],
      "source": [
        "# reset this\n",
        "set_debug(False)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "3e427ad5",
      "metadata": {
        "id": "3e427ad5"
      },
      "outputs": [],
      "source": [
        "# enable tracing and set project name\n",
        "os.environ['LANGCHAIN_TRACING_V2'] = \"false\"\n",
        "\n",
        "# uncomment the following two lines before running the cell if you have a Langchain/Langsmith API Key\n",
        "#os.environ['LANGCHAIN_API_KEY'] = userdata.get('LANGCHAIN_API_KEY')\n",
        "#os.environ['LANGCHAIN_TRACING_V2'] = \"true\"\n",
        "\n",
        "# set langchain project\n",
        "os.environ['LANGCHAIN_PROJECT'] = 'May15'"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "9c9e76df",
      "metadata": {
        "id": "9c9e76df"
      },
      "outputs": [],
      "source": [
        "# use a the basic chain from above\n",
        "chain = (prompt | model | StrOutputParser()) #add new component for tracing\n",
        "response = chain.invoke(\"What is a python f-string?\")\n",
        "response"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "11d4399e",
      "metadata": {
        "id": "11d4399e"
      },
      "source": [
        "#### View langsmith traces\n",
        "We can take a look at this trace on [Langsmith](https://smith.langchain.com)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "545b8841",
      "metadata": {
        "id": "545b8841"
      },
      "source": [
        "## Adding Memory\n",
        "Adapted from: [LCEL Adding Message History](https://python.langchain.com/v0.1/docs/expression_language/how_to/message_history/)\n",
        "Also see:\n",
        "- [Langchain -> Use Cases -> Chatbots -> Memory Management](https://python.langchain.com/v0.1/docs/use_cases/chatbots/memory_management/)\n",
        "- [Components -> More -> Memory](https://python.langchain.com/v0.1/docs/modules/memory/)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "0873b34a",
      "metadata": {
        "id": "0873b34a"
      },
      "outputs": [],
      "source": [
        "from langchain_community.chat_message_histories import ChatMessageHistory\n",
        "from langchain_core.chat_history import BaseChatMessageHistory\n",
        "from langchain_core.runnables.history import RunnableWithMessageHistory"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "9843f11f",
      "metadata": {
        "id": "9843f11f"
      },
      "outputs": [],
      "source": [
        "# create chat template with standard elements\n",
        "model = ChatOpenAI(name='gpt-3.5-turbo')\n",
        "prompt = ChatPromptTemplate.from_messages(\n",
        "    [\n",
        "        (\"system\", brief_system_message),\n",
        "        MessagesPlaceholder(variable_name=\"chat_history\"),\n",
        "        (\"human\", \"{most_recent_user_message}\"),\n",
        "    ]\n",
        ")\n",
        "\n",
        "turns_chain = prompt | model | StrOutputParser()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "37b39d13",
      "metadata": {
        "id": "37b39d13"
      },
      "outputs": [],
      "source": [
        "# quickly try out chain, pretending we've already said something to the system\n",
        "first_chat_turn_messages = [(\"human\", \"Tell me a joke about cats\"),\n",
        "                            (\"ai\", \"Cats jump on beds\")]\n",
        "\n",
        "next_user_message = \"What was funny about that joke?\"\n",
        "turns_chain.invoke({'most_recent_user_message': next_user_message,\n",
        "                    'chat_history': first_chat_turn_messages})"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "2f37fa72",
      "metadata": {
        "id": "2f37fa72"
      },
      "outputs": [],
      "source": [
        "# all saved conversations\n",
        "chat_conversation_threads = {}\n",
        "\n",
        "# define function to create new conversation or load old one based on session_id\n",
        "def get_session_history(session_id: str) -> BaseChatMessageHistory:\n",
        "    if session_id not in chat_conversation_threads:\n",
        "        chat_conversation_threads[session_id] = ChatMessageHistory()\n",
        "    return chat_conversation_threads[session_id]\n",
        "\n",
        "# create chat history enabled chain\n",
        "chat_with_message_history = RunnableWithMessageHistory(\n",
        "    turns_chain,\n",
        "    get_session_history,\n",
        "    input_messages_key=\"most_recent_user_message\",\n",
        "    history_messages_key=\"chat_history\",\n",
        ").with_config(run_name = 'Chat with Message History')"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "4ac18605",
      "metadata": {
        "id": "4ac18605"
      },
      "source": [
        "Let's try it!"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "076cfa99",
      "metadata": {
        "id": "076cfa99"
      },
      "outputs": [],
      "source": [
        "# send first message\n",
        "user_message_1 = \"Tell me a joke about cats\"\n",
        "session_id_1 = \"convo_1\"\n",
        "chat_with_message_history.invoke({'most_recent_user_message': # add first message},\n",
        "                                config={\"configurable\": {\"session_id\": # add session_id}})"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "441d9331",
      "metadata": {
        "id": "441d9331"
      },
      "outputs": [],
      "source": [
        "# send second message\n",
        "chat_with_message_history.invoke({'most_recent_user_message': # add another message to the chat},\n",
        "                                    config={\"configurable\": {\"session_id\": session_id_1}})"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "47da336f",
      "metadata": {
        "id": "47da336f"
      },
      "source": [
        "#### View langsmith traces\n",
        "We can take a look at this trace on [Langsmith](https://smith.langchain.com)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "j-T1I1nDiM-A",
      "metadata": {
        "id": "j-T1I1nDiM-A"
      },
      "source": [
        "# Retrieval Augmented Generation (RAG)\n",
        "## Review\n",
        "* Conceptual and step-by-step guide about [RAG](https://python.langchain.com/v0.1/docs/use_cases/question_answering/)\n",
        "* Learn more about implementing [RAG](https://python.langchain.com/docs/expression_language/cookbook/retrieval)\n",
        "\n",
        "**Data Ingestion (Creating a Vector Store of Documents)**\n",
        "<figure>\n",
        "<img src='https://python.langchain.com/v0.1/assets/images/rag_indexing-8160f90a90a33253d0154659cf7d453f.png' height=300/>\n",
        "    <figcaption>\n",
        "        Source: Data Ingestion (Preparing Embeddings), from <a href=https://python.langchain.com/v0.1/docs/use_cases/question_answering/>Langchain Use Case: Q&A with RAG</a>\n",
        "    </figcaption>\n",
        "</figure>\n",
        "\n",
        "**Retrieval and Generation**\n",
        "<figure>\n",
        "<img src='https://python.langchain.com/v0.1/assets/images/rag_retrieval_generation-1046a4668d6bb08786ef73c56d4f228a.png' height=300/>\n",
        "    <figcaption>\n",
        "        Source: Retrieval and Generation, from <a href=https://python.langchain.com/v0.1/docs/use_cases/question_answering/>Langchain Use Case: Q&A with RAG</a>\n",
        "    </figcaption>\n",
        "</figure>\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "HOs-D7XQlmzt",
      "metadata": {
        "id": "HOs-D7XQlmzt"
      },
      "source": [
        "## Document Loaders and Splitters\n",
        "[Data Ingestion/Vector Store Preparation Guide ](https://python.langchain.com/docs/modules/data_connection/)\n",
        "<figure>\n",
        "<img src='https://python.langchain.com/v0.1/assets/images/data_connection-95ff2033a8faa5f3ba41376c0f6dd32a.jpg' height=300/>\n",
        "    <figcaption>\n",
        "        Langchain Retrieval Component, from <a href=https://python.langchain.com/docs/modules/data_connection/>Langchain Components</a>\n",
        "    </figcaption>\n",
        "</figure>\n",
        "\n",
        "**Other extremely useful resources**:\n",
        "* **[Components -> Retrieval -> Document Loaders](https://python.langchain.com/v0.1/docs/modules/data_connection/document_loaders/)**: Use the sidebar to navigate through different types of document loaders. For all available integrations available through langchain, see [Components -> Integrations -> Components](https://python.langchain.com/v0.1/docs/integrations/document_loaders/)\n",
        "* **[Components -> Retrieval -> Text Splitters](https://python.langchain.com/v0.1/docs/modules/data_connection/document_transformers/)**: Use the sidebar to navigate through different types of text splitters. For all available integrations available through langchain, see [Components -> Integrations -> Components](https://python.langchain.com/v0.1/docs/modules/data_connection/document_transformers/)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "88cf0508",
      "metadata": {
        "id": "88cf0508"
      },
      "outputs": [],
      "source": [
        "# example pdf links\n",
        "doc_1 = 'https://registrar.vanderbilt.edu/documents/Undergraduate_School_Catalog_2023-24_UPDATED2.pdf'\n",
        "doc_2 = 'https://www.tnmd.uscourts.gov/sites/tnmd/files/Pro%20Se%20Nonprisoner%20Handbook.pdf'\n",
        "doc_3 = 'https://www.uscis.gov/sites/default/files/document/guides/M-654.pdf'"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "83aff93e",
      "metadata": {
        "id": "83aff93e"
      },
      "source": [
        "### Example: pdfloader and recursive character splitter"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "Sk4B1Ihklod8",
      "metadata": {
        "id": "Sk4B1Ihklod8"
      },
      "outputs": [],
      "source": [
        "from langchain_community.document_loaders import PyPDFLoader\n",
        "from langchain.text_splitter import RecursiveCharacterTextSplitter"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "b31d1836",
      "metadata": {
        "id": "b31d1836"
      },
      "outputs": [],
      "source": [
        "loader = #choose loader and document\n",
        "\n",
        "# Add the kind of text splitter\n",
        "text_splitter = RecursiveCharacterTextSplitter(\n",
        "    chunk_size=800,\n",
        "    chunk_overlap=250,\n",
        ")\n",
        "\n",
        "# use the text splitter to split the document\n",
        "chunks = # load and split chunks"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "1f9490d7",
      "metadata": {
        "id": "1f9490d7"
      },
      "outputs": [],
      "source": [
        "# see how many chunks were made\n",
        "print(len(chunks))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "c7e3261f",
      "metadata": {
        "id": "c7e3261f"
      },
      "outputs": [],
      "source": [
        "# inspect a single chunk\n",
        "chunks[0]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "08ab1136",
      "metadata": {
        "id": "08ab1136"
      },
      "outputs": [],
      "source": [
        "# view first 3 chunks\n",
        "for chunk_index, chunk in enumerate(chunks[:3]):\n",
        "    print(f'****** Chunk {chunk_index} ******\\n{chunk.page_content}\\n')"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "b1fd5bbb",
      "metadata": {
        "id": "b1fd5bbb"
      },
      "source": [
        "### Example: Loading website data and splitting"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "5f15b971",
      "metadata": {
        "id": "5f15b971"
      },
      "outputs": [],
      "source": [
        "from bs4 import SoupStrainer\n",
        "from langchain_community.document_loaders import WebBaseLoader"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "8fa1ea14",
      "metadata": {
        "id": "8fa1ea14"
      },
      "outputs": [],
      "source": [
        "constitution_website = \"https://constitutioncenter.org/the-constitution/full-text\"\n",
        "\n",
        "# load using WebBaseLoader\n",
        "web_loader = WebBaseLoader(constitution_website,\n",
        "                       bs_kwargs = {'parse_only':SoupStrainer(['article'])})\n",
        "\n",
        "# read the document from the website (without splitting)\n",
        "web_document = #load document"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "c754f1f5",
      "metadata": {
        "id": "c754f1f5"
      },
      "outputs": [],
      "source": [
        "# only the first few characters\n",
        "print(web_document[0].page_content[:330])"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "330f96da",
      "metadata": {
        "id": "330f96da"
      },
      "source": [
        "Now, we'll split in a slightly different way. Since we've already scraped the website, we will just directly use the splitter. Note that after we load the website, we have a data type of (list of) `Document`."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "e15cfb1c",
      "metadata": {
        "id": "e15cfb1c"
      },
      "outputs": [],
      "source": [
        "website_splitter = RecursiveCharacterTextSplitter(chunk_size=330, chunk_overlap=100, # add ability to use start_index\n",
        "website_chunks = website_splitter.split_documents(web_document)\n",
        "len(website_chunks)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "f581f865",
      "metadata": {
        "id": "f581f865"
      },
      "outputs": [],
      "source": [
        "website_chunks[:3]"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "841ab566",
      "metadata": {
        "id": "841ab566"
      },
      "source": [
        "If you know less about the constitution and more about Star wars (or another topic available on Wikipedia), feel free to run the cells below to use that text moving forward. It will replace the `website_chunks` variable. You may need to adjust the `chunk_size` and `chunk_overlap` options. Uncomment and run these cells."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "08bb3ff8",
      "metadata": {
        "id": "08bb3ff8"
      },
      "outputs": [],
      "source": [
        "# alternate data\n",
        "webloader = WebBaseLoader('https://simple.wikipedia.org/wiki/Star_Wars_Episode_IV:_A_New_Hope',\n",
        "                       bs_kwargs = {'parse_only':SoupStrainer('div', id='bodyContent')})\n",
        "web_chunks = webloader.load_and_split(RecursiveCharacterTextSplitter(chunk_size=500, chunk_overlap=100, add_start_index=True))\n",
        "print('Number of chunks generated: ', len(web_chunks))\n",
        "print('\\n\\nSample: ')\n",
        "web_chunks[:5]"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "mKdzgo6ci0MD",
      "metadata": {
        "id": "mKdzgo6ci0MD"
      },
      "source": [
        "## Vector Stores: A way to store embeddings (hidden states) of your data\n",
        "The choice of vector store influences how \"relevant\" documents can be identified, speed of document retrieval, and organization.\n",
        "\n",
        "Helpful resources:\n",
        "* **[Brief Langchain Reference](https://python.langchain.com/v0.1/docs/modules/data_connection/vectorstores/)**\n",
        "* **[Vector Store Integrations](https://python.langchain.com/v0.1/docs/integrations/vectorstores/)**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "2831d1dd",
      "metadata": {
        "id": "2831d1dd"
      },
      "outputs": [],
      "source": [
        "from langchain_community.vectorstores import Chroma\n",
        "from langchain_openai import ChatOpenAI, OpenAIEmbeddings"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "jmXDTgoqjCUs",
      "metadata": {
        "id": "jmXDTgoqjCUs"
      },
      "outputs": [],
      "source": [
        "# create the vector store\n",
        "db = # code to create store"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "217d27ea",
      "metadata": {
        "id": "217d27ea"
      },
      "source": [
        "### Similarity Search for Documents"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "XC-5zEeioFmq",
      "metadata": {
        "id": "XC-5zEeioFmq"
      },
      "outputs": [],
      "source": [
        "# query the vector store\n",
        "query = 'When was a new hope released?'\n",
        "\n",
        "# use a similarity search between the vectors\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "8d1c197d",
      "metadata": {
        "id": "8d1c197d"
      },
      "outputs": [],
      "source": [
        "# get cosine distance alongside results\n",
        "relevant_docs =\n",
        "relevant_docs"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "CWJwyByyhoSB",
      "metadata": {
        "id": "CWJwyByyhoSB"
      },
      "outputs": [],
      "source": [
        "# another query, but instead use normalized score\n",
        "query = 'What is the plot of A New Hope?'\n",
        "relevant_docs = # use normalized score\n",
        "relevant_docs"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "lt_uoYfLjUdx",
      "metadata": {
        "id": "lt_uoYfLjUdx"
      },
      "source": [
        "## Retrievers: How we select the most relevant data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "KTNxqQf7kiY_",
      "metadata": {
        "id": "KTNxqQf7kiY_"
      },
      "outputs": [],
      "source": [
        "# or use the db as a retriever with lcel\n",
        "retriever = # create retriever\n",
        "retrieved_docs = retriever.invoke(query)\n",
        "retrieved_docs"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "8IuaToUPc0Hy",
      "metadata": {
        "id": "8IuaToUPc0Hy"
      },
      "source": [
        "## RAG\n",
        "For when we want to actually do generation, but want there to be retrieved documents included in the generation. For this, we're going to switch to a different embedding model which will be downloaded on our machine (or if on Google Colab, there)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "d3298219",
      "metadata": {
        "id": "d3298219"
      },
      "outputs": [],
      "source": [
        "from langchain_community.embeddings import HuggingFaceEmbeddings\n",
        "from langchain_community.vectorstores import FAISS\n",
        "\n",
        "from langchain_core.runnables import RunnableLambda, RunnablePassthrough, RunnableSequence, RunnableParallel"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "d4f952c1",
      "metadata": {
        "id": "d4f952c1"
      },
      "outputs": [],
      "source": [
        "# use different embedding model\n",
        "embeddings_fn = HuggingFaceEmbeddings(model_name=\"all-MiniLM-L6-v2\") #, model_kwargs={\"device\":'mps'})\n",
        "hf_db = FAISS.from_documents(web_chunks, embeddings_fn)\n",
        "hf_retriever = hf_db.as_retriever(search_kwargs={\"k\":1})\n",
        "\n",
        "# make sure it works\n",
        "query = 'What is the plot of A New Hope?'\n",
        "hf_retriever.invoke(query)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "16cb408b",
      "metadata": {
        "id": "16cb408b"
      },
      "source": [
        "### Default RAG: Question Answering"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "Fsewtgf9alyl",
      "metadata": {
        "id": "Fsewtgf9alyl"
      },
      "outputs": [],
      "source": [
        "# Basic question answering template\n",
        "template = \"\"\"Answer the question based only on the following context:\n",
        "{context}\n",
        "\n",
        "Question: {question}\n",
        "\"\"\"\n",
        "\n",
        "# compose prompt\n",
        "rag_prompt = ChatPromptTemplate.from_template(template)\n",
        "\n",
        "# create model (so we don't have to depend on the model definition at the top of the notebook)\n",
        "model = ChatOpenAI(model_name='gpt-3.5-turbo')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "8ExSC1rNNmCw",
      "metadata": {
        "id": "8ExSC1rNNmCw"
      },
      "outputs": [],
      "source": [
        "# We need to format the retrieved documents better\n",
        "def format_docs(docs):\n",
        "    return \"\\n\\n\".join([f'Reference text:\\n{doc.page_content}\\n\\Citation Info: {doc.metadata}' for doc in docs])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "73636824",
      "metadata": {
        "id": "73636824"
      },
      "outputs": [],
      "source": [
        "# inspect behavior of format_docs\n",
        "format_docs(web_chunks[:3])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "21071aec",
      "metadata": {
        "id": "21071aec"
      },
      "outputs": [],
      "source": [
        "# compose the chain\n",
        "rag_chain = (\n",
        "    ## Add special rag part\n",
        "    | rag_prompt\n",
        "    | model\n",
        "    | StrOutputParser()\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "e2cba6e1",
      "metadata": {
        "id": "e2cba6e1"
      },
      "outputs": [],
      "source": [
        "# run the chain\n",
        "rag_chain.with_config(run_name = 'basic_rag_chain').invoke('What is the plot of a new hope')"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "31c99c95",
      "metadata": {
        "id": "31c99c95"
      },
      "source": [
        "### RAG with Sources\n",
        "Resource: [Langchain: Returning Sources](https://python.langchain.com/v0.1/docs/use_cases/question_answering/sources/)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "d47c733e",
      "metadata": {
        "id": "d47c733e"
      },
      "outputs": [],
      "source": [
        "from langchain_core.runnables import RunnableParallel"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "7fdb371f",
      "metadata": {
        "id": "7fdb371f"
      },
      "outputs": [],
      "source": [
        "# Basic prompt -> model -> parser chain\n",
        "single_turn_chain = (\n",
        "    rag_prompt\n",
        "    | model\n",
        "    | StrOutputParser()\n",
        ")\n",
        "\n",
        "# Break previous chain in half to access context and question in returned response\n",
        "rag_chain_with_source = RunnableParallel(\n",
        "    {\"context\": hf_retriever | format_docs, \"question\": RunnablePassthrough()}\n",
        ").assign(answer=single_turn_chain)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "e4b36d6e",
      "metadata": {
        "id": "e4b36d6e"
      },
      "outputs": [],
      "source": [
        "# invoke\n",
        "response = rag_chain_with_source.with_config(run_name = 'sources_rag_chain').invoke(\"What happened to Princess Leia in a New Hope?\")\n",
        "\n",
        "# print full response\n",
        "for key, value in response.items():\n",
        "    print(f\"{key}: {value}\\n\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "afec178a",
      "metadata": {
        "id": "afec178a"
      },
      "source": [
        "### RAG with Chat History?\n",
        "\n",
        "We will have a one-turn system with our RAG system. How do we add chat memory? See below for implementation guides:\n",
        "- [Use cases: Q&A with Rag: Add Chat History.](https://python.langchain.com/v0.1/docs/use_cases/question_answering/chat_history/)  Builds on a RAG system, so will be of interest."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "a9d3434e",
      "metadata": {
        "id": "a9d3434e"
      },
      "source": [
        "## LLM System Metrics\n",
        "Resource: [Guides -> Evaluation](https://python.langchain.com/v0.1/docs/guides/productionization/evaluation/)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "92372d35",
      "metadata": {
        "id": "92372d35"
      },
      "outputs": [],
      "source": [
        "from langchain.evaluation import load_evaluator"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "c7bac358",
      "metadata": {
        "id": "c7bac358"
      },
      "outputs": [],
      "source": [
        "# configure what we want to evaluate\n",
        "rs_question = \"What happened to Princess Leia in a New Hope?\"\n",
        "rs_answer = rag_chain.with_config(run_name = 'basic_rag_chain').invoke(rs_question)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "1da42e15",
      "metadata": {
        "id": "1da42e15"
      },
      "outputs": [],
      "source": [
        "# load an evaluator that uses the conciseness criteria\n",
        "evaluator = #load evaluator\n",
        "\n",
        "# evaluate whether our model was concise or not\n",
        "eval_result = evaluator.evaluate_strings(\n",
        "    # add inputs to evaluate\n",
        ")\n",
        "\n",
        "# print result\n",
        "eval_result"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "933a5e2e",
      "metadata": {
        "id": "933a5e2e"
      },
      "source": [
        "View other criteria available through LangChain"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "e6dbb810",
      "metadata": {
        "id": "e6dbb810"
      },
      "outputs": [],
      "source": [
        "from langchain.evaluation import Criteria\n",
        "list(Criteria)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "34ea2044",
      "metadata": {
        "id": "34ea2044"
      },
      "source": [
        "# Homework\n",
        "The following exercises are designed to help you gain depth in what you've learned about RAG today.\n",
        "\n",
        "## [Required] Learning more about RAG\n",
        "### Splitting Text (Conceptual)\n",
        "There are so many ways to split the text, and each has an impact on the resultant RAG system. Below is a resource (with sidebar dropdown) for you to read over and then answer the following question for the text splitting approaches (as relevant to your application):\n",
        "* What is the proposed value in adopting this text splitting approach? What are some drawbacks?\n",
        "\n",
        "### Splitting Text (Programmatic)\n",
        "Above, we have adopted specific chunk sizes and splitting approaches. Choose one of the documents (or use your own) and:\n",
        "* Modify the chunk size. How does this impact the resulting RAG performance? The cost?\n",
        "* Implement a different type of text splitter (as applicable, i.e., not code text splitters if you're not splitting code). How does this impact the resulting RAG performance? The cost?\n",
        "\n",
        "### Customizing RAG\n",
        "There are many, many, many ways to improve results with RAG. Below are some resources for you to read over then complete the following:\n",
        "1. What is the proposed value in adopting this approach? In other words, what is the expected performance improvement by using this method?\n",
        "2. How might it apply to your work?\n",
        "\n",
        "* [**Query Analysis**](https://python.langchain.com/v0.1/docs/use_cases/query_analysis/). Make sure to peruse subtopics.\n",
        "* [**Synthetic Data Generation**](https://python.langchain.com/v0.1/docs/use_cases/data_generation/).\n",
        "* [**Tagging**](https://python.langchain.com/v0.1/docs/use_cases/tagging/).\n",
        "* [**Routing Chain Logic Based on inputs****](https://python.langchain.com/v0.1/docs/expression_language/how_to/routing/).\n",
        "* [**Chain Composition**](https://python.langchain.com/v0.1/docs/modules/chains/). Of particular interest here are the Legacy chains. Although they will probably be completely removed in the future, consider their behavior. In what cases might these behaviors be useful?\n",
        "\n",
        "** This is highly recommended reading, but may not be suitable for those who are novices in programming. Although there is text, the code demonstrates concretely by the text. For novices, it may be better to copy/paste the code as well to understand the behavior, although it is noted that such a task may be outside of the the time constraints of for some participants.\n",
        "\n",
        "## [Required] Learning more about Evaluation\n",
        "Read the following text and answer these questions:\n",
        "1. What is the purpose of the individual criterion? Does it require and external LLM for evaluation?\n",
        "2. In what cases might this criteria be useful?\n",
        "\n",
        "Depth Text: [**Evaluation, by Langchain**](https://python.langchain.com/v0.1/docs/guides/productionization/evaluation/)\n",
        "\n",
        "## [Highly recommended] Learning more about the LLM System Lifecycle\n",
        "There is more to an LLM-based system than a user interface and the LLM chain. There is a whole framework around inspecting, testing, and evaluating these systems. Read the following and answer the questions below:\n",
        "1. Summarize the purpose of the individual components of the langsmith system (they generalize to all LLM systems).\n",
        "2. Consider your favorite LLM UI (i.e., ChatGPT, Gemini, Claude, etc). Describe how you think these components are utilized the LLM system.\n",
        "\n",
        "Depth Text: [**LangSmith User Guide**](https://docs.smith.langchain.com/old/user_guide)\n",
        "\n",
        "## [Recommended] Practicing with RAG and Langchain\n",
        "### Exercise 1: Modify the RAG system\n",
        "Modify or create a new chain which:\n",
        "1. Uses a different LLM than the one used in this notebook.\n",
        "2. Uses a different document loader\n",
        "3. Uses a different splitter than the one used in this notebook.\n",
        "4. Uses a different vector store/retriever than the one used in this notebook.\n",
        "\n",
        "Use the resources provided in the relevant sections of the notebook for other options.\n",
        "\n",
        "### Exercise 2: Implement a new RAG system\n",
        "1. [More challenging] Add chat history to one of your RAG chains. Make sure to enable tracing and inspect langsmith to ensure that the chat history is used.\n",
        "2. Create a gradio user interface to use your chain in a more user-friendly way.\n",
        "3. [Challenging] Implement an additional chain which uses one of the strategies you read about in the \"Learning more about RAG\" section.\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "python3",
      "name": "python3"
    },
    "colab": {
      "provenance": [],
      "toc_visible": true,
      "include_colab_link": true
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}